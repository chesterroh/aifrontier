---
episodeNumber: 84
title: "Physical AI를 알아보자 (sudoremove 박종현 대표)"
description: "Physical AI와 VLA(Vision-Language-Action) 모델이 왜 지금 로보틱스의 핵심 화두인지, sudoremove 박종현 님과 함께 최신 데모들을 바탕으로 큰 흐름을 정리합니다."
publishedAt: 2026-02-11
duration: "1:24:02"
youtubeId: "MZdhBBvUmvU"
thumbnail: "https://i.ytimg.com/vi/MZdhBBvUmvU/maxresdefault.jpg"
hosts:
  - 노정석
  - 박종현
chapters:
  - time: "00:00"
    title: "인트로 및 게스트 소개: 박종현(sudoremove)"
  - time: "02:26"
    title: "최신 로봇 데모: Boston Dynamics Atlas와 지능의 의미"
  - time: "05:14"
    title: "촉각까지 붙는 VTLA: Sharpa CraftNet"
  - time: "06:12"
    title: "Figure Helix: end-to-end 제어의 등장"
  - time: "08:17"
    title: "Physical AI의 정의와 범위"
  - time: "10:49"
    title: "안 되던 게 되는 시대: 빨래 개기와 deformable object"
  - time: "14:45"
    title: "Specialist에서 Generalist로: Robot Foundation Model"
  - time: "17:53"
    title: "Physical Intelligence π0.5: 일반화 데모"
  - time: "19:55"
    title: "VLA 용어 정리: RFM, VLA, LBM"
  - time: "24:01"
    title: "핵심 병목: 행동 데이터는 인터넷에 없다"
  - time: "25:55"
    title: "데이터 수집의 현실: 텔레오퍼레이션과 다양한 접근법"
  - time: "29:20"
    title: "시뮬레이션 기반 접근: NVIDIA Cosmos와 Sim-to-Real Gap"
  - time: "38:46"
    title: "Scaling Law와 타임라인 전망"
  - time: "44:04"
    title: "VLA 모델의 수렴 진화와 남은 논쟁들"
  - time: "48:04"
    title: "VLA 계보: System 1/2 아키텍처"
  - time: "55:59"
    title: "연속 행동과 Diffusion 기반 접근"
  - time: "63:48"
    title: "Physical Intelligence란 무엇인가: 모라벡의 역설"
  - time: "74:27"
    title: "사업 방향: 커뮤니티 전략과 게임 시뮬레이션"
  - time: "77:10"
    title: "입문 가이드: LeRobot, Physical Intelligence 논문"
  - time: "79:45"
    title: "로봇이 우리 삶에 들어오는 미래와 마무리"
lang: "ko"
alternateSlug: null
notionUrl: "https://sudoremove.com/knowledge/essays/archive/physical-ai-is-coming/"
---

## 인트로 및 게스트 소개: 박종현(sudoremove)    *00:00*

<span class="paragraph-timestamp" data-ts="00:00">00:00</span> **노정석** 이 녹화를 하고 있는 오늘은 2026년 1월 31일 토요일 아침입니다. 오늘은 제가 정말 좋아하는 채널이고 요새 너무너무 많이 보고 있는데요. sudoremove 채널의 박종현 님 모셨습니다.

박종현 님과 저희가 몇 번 미팅을 가졌는데 지금 최대 관심사가 Physical AI거든요. 이 Physical AI에서 어떠한 기회가 있을 것 같다라는 그런 말씀을 만날 때마다 계속 말씀을 주시고 그 부분을 계속 트래킹을 해 오셔서요. 저희가 오늘 종현 님을 통해서 이 VLA 개론에 대해서 학습을 하고 어떤 일이 일어나는지, 그리고 안은 어떻게 생겼는지 그리고 여기서 생각해 봐야 될 점들은 무엇인지 이런 것들을 한번 들어보고자 합니다. 선생님으로 모셨습니다. 어서 오십시오.

<span class="paragraph-timestamp" data-ts="00:49">00:49</span> **박종현** 안녕하세요. 제가 일단 선생님은 아니고요. 제가 많이 배우고 있습니다. 저는 항상 모든 분들한테 다 같이 가는 도반들이다 이렇게 표현을 하는데 저도 부족한 점이 많기 때문에 우선 저희 같은 경우에는 저는 한 1년 정도 된 것 같아요.

그 당시 1년 전에 LLM 열심히 팔로우업하고 있었고 그때 DeepSeek R1이 나오면서 reasoning 모델 이런 거 저도 그때 직접 만들어보고 이런 일을 하고 있었는데 그때 처음 VLA라는 걸 한번 돌려보면서 아 이게 가능성이 있구나 이런 생각을 하게 됐고 그러고 나서 작년을 돌이켜 보면 이 Physical AI라는 키워드가 언론에서도 많이 떠오르게 된 것 같아요.

사실은 NVIDIA가 밀어서라고 저는 생각을 하는데 왜 이런 키워드가 뜰까 생각을 해보면 지금 저희 세상에 어쨌든 LLM이 나오고 이 AGI라는 시대가 올 것 같다 이런 생각들을 저희가 하고 있는데 지능을 조금만 더 나눠서 생각을 해보면 LLM이 하고 있는 일들은 거의 대부분 코딩, 수학, 추론 이런 부분들에 다 좀 집중이 되어 있습니다.

그런데 이 지능이랑 저희가 물리적인 행동을 하는 지능이랑은 조금 다른 면이 있다고 저희가 팔로우업을 하면서 느꼈어요.

그래서 이런 물리적인 지능에 대해서 기존의 LLM이 풀고 있는 그런 지능적인 행위들은 제가 임의로 Cognitive Intelligence 인지적인 지능이라고 분류를 했고 그거 옆에 있는 Physical Intelligence는 어떤 특징이 있는가 우리는 이걸 어떻게 풀어나가고 있는가 이런 이야기를 한번 해보도록 하겠습니다.

<span class="paragraph-timestamp" data-ts="02:24">02:24</span> **노정석** 오늘 너무 재밌을 것 같습니다.

## 최신 로봇 데모: Boston Dynamics Atlas와 지능의 의미    *02:26*

<span class="paragraph-timestamp" data-ts="02:26">02:26</span> **박종현** 우선 첫 번째는 데모부터 보려고 하는데 팟캐스트로 들으시는 분들이 많아서 제가 영상이 많이 있다 보니까 로봇은 영상이 많아서 가능하시면 화면으로 보시는 걸 추천드리고요. 우선 이번 CES에 나온 가장 핫한 데모죠. 이거는 우리나라에서만 그런 게 아니고 전 세계에서 가장 핫한 데모가 이 Boston Dynamics의 Atlas라는 휴머노이드가 나와서 이런 움직임들을 보여줬거든요.

<span class="paragraph-timestamp" data-ts="02:56">02:56</span> **노정석** 놀라웠죠.

<span class="paragraph-timestamp" data-ts="03:03">03:03</span> **박종현** 특히 이게 가장 인기가 많은 모션이었던 것 같아요. 이런 사람과 같은 웨이브스러운 움직임 이런 것들과 새로운 몸체 이런 것들을 보여주면서 실제로 모회사인 현대의 주가가 엄청 오르기도 했고요. 주목을 많이 받았는데 이런 행동들을 한번 보면 여기에는 과연 어떤 지능이 있을까 이런 생각을 해보면 제가 생각했을 때는 공개된 게 아니라서 확실하게 알 수 없습니다만 사실 별다른 지능이 없다고 생각이 돼요. 데모를 보면 그냥 일어나고요. 걷고요. 똑같은 동작을 보여줍니다. 그래서 이거는 뭔가 이럴 땐 이렇게 하고 저럴 땐 저렇게 하고, 넘어질 것 같으면 갑자기 중심 잡고, 아무런 물체가 날아와도 손으로 반응해서 잡고 이런 거랑은 다르거든요. 그래서 사실 옛날에도 배웠던 그런 지능이 있는 그런 파트는 아니라고 생각을 하고요. 다만 이 몸체 자체가 너무나 훌륭해서 사람들이 주목을 했던 것 같습니다.

그런데 이 Boston Dynamics의 Atlas가 사실 이 CES 데모에서는 그런 지능이 필요한 데모를 보여주진 않았지만 작년에 연구가 나왔었어요. 이 Atlas를 가지고 지능이 있어야만 수행 가능한 일들을 이렇게 보여줬거든요. 실제로 노동을 하는 거예요. 지능이 필요한 게 뭐냐 하면 저렇게 옆에 그냥 뭐가 떨어져 있는 거예요. 그러니까 어떤 물체가 어떤 방식으로 상호작용을 할지 모르는데 그런 상황에 대해서 다 맞춰서 동작을 하는 거죠. 보면 Boston Dynamics라 그런지 하키채로 이렇게 괴롭히거든요. 아무리 괴롭혀도 다 알아서 동작을 하는 거예요. 사실 이게 사람한테는 되게 쉬운 일인데 이런 동적인 상황을 다 커버하는 게 원래 안 되던 일입니다. 이런 게 될 것 같다. 그래서 어떤 물체가 있어도 알아서 잡고 접고 노동을 시키는 거죠. 이거 Atlas가 돌아가고 있는 이 형태 같은 경우에는 LBM이라는 모델이 들어 있거든요. VLA랑 비슷한 거예요.

모회사가 현대긴 하지만 이거는 Toyota Research Institute 연구 기관이랑 함께 해서 LBM을 만들어서 여기에 이 모델을 올려서 동작시키는 데모였고요. 어쨌든 이런 것들이 결국에는 지능을 요하는 일이고 하나둘씩 나오고 있다.

## 촉각까지 붙는 VTLA: Sharpa CraftNet    *05:14*

<span class="paragraph-timestamp" data-ts="05:19">05:19</span> **박종현** 그래서 이번 CES에서 나온 로봇과 모델들 중에 저희가 가장 인상 깊게 봤던 거는 이 Sharpa라는, 저도 처음 들어봤어요. 이런 회사에서 CraftNet이라는 VLA가 나왔고 이걸로 뭘 보여줬냐면 이런 트럼프 카드를 딜링하는 걸 보여줬거든요. 이게 제가 봤을 때는 아마도 첫 번째 데모예요. 이 트럼프 카드를 딜링하는 손. 이 CraftNet이라는 거는 제가 VLA라고 했는데 또 이름을 이렇게 붙였습니다. VTLA라고 부르는데 tactile, 그러니까 촉각이 있는 데모예요. 그래서 앞에 데모를 보면 바람개비 종이접기 하듯이 바람개비를 접는 데모가 있습니다. 이게 촉각이 없이는 상당히 수행하기가 어려운 데모거든요. 촉각까지 받아서 눈, 언어, 촉각 이런 것들을 통해서 행동을 만들어내는 모델이 처음으로 나오기 시작했다. 이게 현 시점인 것 같고요.

## Figure Helix: end-to-end 제어의 등장    *06:12*

<span class="paragraph-timestamp" data-ts="06:15">06:15</span> **박종현** 따끈따끈한 몸체를 하나만 더 보자면 Figure라는 상당히 투자를 많이 받은 회사에서 지난주에 올라온 Helix라는 모델인데 주방에서 그냥 일을 시킵니다. 그럼 걸어 다니면서 물건을 집어서 정리를 하고 이런 걸 합니다. 그런데 여기서 제가 하나만 보자면 이거 한 번만 보겠습니다. 저렇게 골반으로 쳐 가지고 넣는 걸 보면서 아 이게 진짜 사람처럼 학습을 잘 시켰구나. 지금 이 데모는 autonomous하다고 써져 있고요.

그런데 제가 조금만 생각을 해보자면 사실 텔레오퍼레이션으로 하는 거랑 이런 한 4분 정도 되거든요. 4분이 좀 안 되는 데모랑 이렇게 발로 드는 것도 그렇고 저희는 이렇게 얘기합니다. 4분 정도의 데모는 고급 텔레오퍼레이션이랑 다를 바가 없다. 그냥 저희가 몇 백 번 텔레오퍼레이션을 해서 저대로 동작 데이터를 딴 다음에 그대로 학습을 시키면 사실은 크게 다를 바가 없다 이렇게 얘기를 하긴 합니다. 근데 그래도 대단한 겁니다. 전신 텔레오퍼레이션을 사람처럼 동작하는 모델을 만들었다는 것 자체가. 일단 이 정도. 그래서 얘네가 자랑하고 싶었던 거는 4분이면 그래도 꽤 길게 연속 동작을 다 해냈다는 거고 그다음에 사람들이 놀랐던 거는 저렇게 골반을 치는 거나 발로 들거나 하는 인간처럼 행동하는 모습을 보여줬다는 거.

그리고 자랑하고 싶었던 거는 그 문서에는 한 10만 줄 되는 C++로 짜져 있는 그런 low-level한 control이 다 그냥 end-to-end로 모델이 다 대체를 해버렸다. 이게 결국에는 Tesla의 FSD가 자율주행에서도 자랑하고 싶어 하는 거랑 사실 똑같은 거죠. end-to-end로 코드가 없어진다. rule-based 때 어떤 로직들이 다 없어진다. 이런 방향으로 나아가는 것 같습니다.

<span class="paragraph-timestamp" data-ts="08:03">08:03</span> **노정석** Tesla가 정확하게 보여줬었던 그 trajectory죠. 그 자취죠.

<span class="paragraph-timestamp" data-ts="08:13">08:13</span> **박종현** 그 길을 그대로 다 로봇에서도 똑같이 가고 있는 것 같고요. 일단 여기까지가 어쨌든 가장 최신 데모들을 살짝 한번 봤고요. 그래서 이 Physical AI라는 키워드가 너무나 많이 밖에서 쓰고 있는데 일단 저희는 오늘 하루 동안은 Physical AI를 정의를 조금 좁혀 가지고 이런 걸 칭하겠다 하는 걸 정리를 하고 가겠습니다.

## Physical AI의 정의와 범위    *08:17*

<span class="paragraph-timestamp" data-ts="08:33">08:33</span> **박종현** 그래서 제가 생각하는 Physical AI라는 게 뭐냐면 옛날에 rule-based로 짜져 있었던 수많은 로직들, 이런 것들이 없어질 것 같거든요. 방금 Helix에서 주장했었던 것처럼 그래서 이 end-to-end 학습을 통해서 어떤 비정형적인 상황들을 다 커버하는 그런 진짜 물리적인 지능들이 들어오는 것 그렇게 해서 바뀌어 나가는 무언가 이거를 Physical AI라고 정의를 하려고 합니다. 그래서 제가 이거는 조금만 더 자세히 한번 보면요.

일단 이 Physical AI라는 용어 자체는 NVIDIA에서 한 2년 전쯤부터 썼던 용어고 로보틱스에서도 ChatGPT 모먼트 같은 게 ChatGPT 모먼트가 곧 올 것 같다 이런 이야기를 했었고 진짜로 그런 것 같다고 저도 생각을 하고 있고요. 그런데 NVIDIA에서는 이 Physical AI라는 용어 자체를 조금 넓게 사용하고 있는 것 같아요. 그냥 물리적인 어떤 행동을 하는 무언가가 다 AI가 들어가 있으면, 이런 휴머노이드뿐만 아니라 이렇게 로봇 팔이 시뮬레이터에서 돌아다니거나 아니면 저희가 음식점에서 많이 보죠. 음식점 메뉴 나오면 배달해 주는 로봇이나 이런 것도 다 Physical AI라고 생각을 하고 있는 것 같은데요.

이것도 당연히 틀린 말은 아닙니다만 저희 채널에서 저희가 관심 있는 거는 그런 것보다는 VLA 혹은 VLA가 아닐 수도 있어요. 어떤 형태로든지 간에 end-to-end로 학습을 해서 general한 일을 할 수 있는 그런 로봇 그런 지능이 탑재된 로봇 이거를 일단은 저희가 Physical AI라고 정의를 하고 이 범위 안에서 오늘의 이야기는 하도록 하겠습니다.

<span class="paragraph-timestamp" data-ts="10:16">10:16</span> **노정석** 좋습니다.

그래서 제가 왜 이렇게 정의를 했냐면 제가 느끼기에는 어떤 명확한 경계점 같은 게 있었던 것 같아요. 우리가 LLM 이전에도 어떤 지능이라고 부를 만한 무언가들이 많이 있었는데 사실 LLM이 나오고 나서 완전히 달라졌잖아요. 그런 것처럼 이 LLM을 기반으로 한 어떤 물리적인 지능의 구현도 완전히 다르다고 생각을 해요. 그럼 뭐가 다르냐라고 하면 단순하게는 사실 Physical AI라는 키워드가 뜨는 이유랑도 똑같은데, 안 되던 게 됩니다. 옛날에 안 됐던 것들.

그래서 옛날에 어떤 게 안 됐었냐면 이런 게 안 됐어요. 빨래를 개는 거. 제가 이건 직접 찍은 거거든요. 그래서 이 데모를 한번 보면 그냥 빨래가 널려 있으면 그 빨래를 자기가 알아서 펴고 접고 잘 개고 이런 걸 합니다. 로봇을 생각해 보면 사실 보행은 예전에도 되긴 됐거든요. 이만큼 잘 되지는 않았지만 근데 보행은 제가 느끼기에는 되게 작은 task예요. 왜냐하면 사실 관절도 움직여야 될 관절이 그렇게 많지 않고요. 그냥 밸런스를 잡아가면서 걸어가는 게 다라고 생각을 하면 사실 보행도 잘 안 되는 게 평평하지 않은 어떤 계단이 있거나 아니면 장애물이 있거나 그 모든 바닥에 대해서 다 대응을 하는 게 되게 어렵거든요. 숲 속에 어떤 진흙이 있거나.

빨래도 사실 마찬가지입니다. 이 속에 지능이 있으면 보행하는 면이 비정형적이에요. 어떤 게 바닥에 있을지 모릅니다. 이 빨래도, 어떤 물체를 옮기는 것도 사실 빨래가 되게 별거 아니라고 느껴지실 수도 있는데 강체를 움직이는 거랑은 완전히 다른 문제예요. 우리가 이런 걸 뭐라고 부르냐면 deformable한 object라고 하거든요. 일단 시뮬레이션이 잘 안 됩니다. 아무 옷이나 접는 거지만 사실 옷이라는 게 이게 흐물흐물하기 때문에 어떤 동작을 했느냐에 따라서 모양이 너무 다양하게 다 바뀌어요. 이런 엄청나게 다양한 경우의 모양에 대해서 다 커버를 할 수 있는 게 사실 지능이 필요한 일이고 그런 deformable한 object를 다루는 게 가능하다.

## 안 되던 게 되는 시대: 빨래 개기와 deformable object    *10:49*

<span class="paragraph-timestamp" data-ts="12:32">12:32</span> **노정석** 그래서 또 어떤 게 가능하냐면 아까 봤던 그 Helix, Helix 두 번째 버전이 이번에 나온 거고 첫 번째 Helix가 나왔을 때 이런 자랑을 했었어요. 물류에서의 어떤 노동을 하는 건데 이게 비닐 박스거든요. 이 비닐 박스가 deformable해요. 그래서 시뮬레이션도 잘 안 되고 computation이 너무 많이 필요한 거죠. 그다음에 안에 뭐가 들어 있을지 모르기 때문에 우리가 집으면 모양이 제멋대로 바뀝니다. 사람은 안에 뭐가 들어있든지 잘 몰라도 이런 걸 너무나 잘 다루거든요. 그래서 이런 데모들이 나오기 시작한 게 여기도 지능이 하나씩 추가가 되는구나 이런 걸 느낄 수가 있었던 것 같고 심지어 이 데모는 한 시간이에요. 짧은 데모는 cherry-pick 하면 보여줄 수 있는데 1시간 동안 하는 데모를 보여주면서 우리 진짜 된다, 이런 것들을 증명했던 사례였던 것 같고요. 실제로 저는 이게 1시간 가까이 거의 열심히 봤는데 다 성공하진 않습니다.

중간에 떨어뜨려서 바닥에 놓치고 그런 것도 하긴 하더라고요. 이때도 보면 상반신만 하긴 하거든요. 그래서 이때는 whole upper body, 상반신만 fully autonomous하게 동작하는 그런 로직이긴 했습니다.

그리고 하나만 더 추가를 하자면 앞에서 뒤에서 할 이야기인데 결국 LLM을 기반으로 얘네들이 거의 다 만들어져 있거든요. 요즘 모델들은 LLM에는 어떤 상식이 있습니다. World Knowledge라는 게 있어서 이 상식이라는 게 통한다. 기존의 모델들은 상식이 없어요. 걷는 거는 상식이 없어도 돼요. 그렇지만 우리가 예를 들어서 빨간 컵을 집어라, 그러면 컵이라는 게 완전히 새로운 모양의 창의적인 컵을 봐도 우리가 컵처럼 생겼으면 컵이라는 걸 다 알거든요. 그런데 예전에는 그런 걸 몰랐죠. 그래서 컵이 무엇인지, 집는 게 무엇인지라는 것 자체에 대한 어떤 상식 같은 게 있기 때문에 어떤 모양의 컵이 와도 우리가 다 집을 수가 있는 거죠. 그래서 이런 상식 같은 게 LLM이 인터넷 스케일의 데이터를 학습하면서 다 가지고 있기 때문에 이런 게 가능해졌다는 겁니다.

그래서 여러 가지 회사들의 접근법들이 나오는데 이건 조금 이따가 다시 한번 보겠습니다. 그래서 안 되던 게 되는 거, 이걸 조금만 더 풀어서 설명을 하자면 specialist였던 모델들이 generalist가 되어서 모든 상황에 다 대처하는 범용 모델이 된다. 제가 vision의 예시를 하나 가져왔는데 vision이고 LLM이고 다 똑같거든요. 옛날을 생각을 해보면 이미지를 하나 주면 이게 뭐야라고 물어보면 그걸 맞추는, classify 하는 모델이 따로 있었습니다. 이게 딥러닝이라는 게 세상에 나오면서 CNN 기반으로 다 풀리게 됐는데, 이미지를 주고 이게 뭐야가 아니라 어디 있어까지 물어보면 object를 detect하는 좌표까지 따는 모델, 제일 유명한 게 YOLO 이런 모델들이 필요했고 저희가 지금도 화상 회의에 Zoom의 배경을 블러 처리를 할 수가 있는데 크로마키라는 게 없어도 얼굴의 윤곽 같은 거를 따서 배경을 블러를 할 수 있잖아요. 그런 segmentation도, segmentation 하는 모델이 따로 있었어요.

language도 마찬가지예요. 번역을 해 달라고 그러면 번역을 하는 모델이 따로 있고요. 이 글이 긍정적인지 부정적인지 sentiment analysis를 해 달라고 하면 그걸 하는 모델이 다 따로 있고 그랬는데 지금 vision도 마찬가지고 language도 마찬가지고 그렇게 모델을 따로따로 하지 않거든요. ChatGPT 키고 이미지 던진 다음에 이거 뭐야 물어보면 대충 알아서 다 잘 설명을 해줍니다. task라는 게 구분이 없어졌어요. 하나의 모델이 모든 걸 다 합니다. GPT가 혹은 우리가 많이 쓰고 있는 LLM이든 VLM이든 이런 친구들이 다 general한 모델인 거죠.

이 로봇도 마찬가지인 거예요. 어떤 특정한 몸체, 그 몸체에다가 포켓볼을 시키려면 거기에 맞게 코딩을 하거나 어떤 rule-based의 모델을 만들거나 그런 걸 해야 돼요. 커피를 만들려고 바리스타로서 쓰려고 해도 그렇게 만들어야 돼요. 똑같은 바리스타 일을 시킨다고 하더라도 몸체가 바뀌면 또 새로 만들어야 돼요. 이게 기존의 로직이었는데 앞으로 하려고 하는 게 뭐냐면 하나의 모델이 어떤 몸체든 어떤 task든 다 하겠다. 그래서 이런 거를 Robot Foundation Model이라고 부를 수 있을 것 같아요.

## Specialist에서 Generalist로: Robot Foundation Model    *14:45*

<span class="paragraph-timestamp" data-ts="16:55">16:55</span> **노정석** LLM도 general한 일을 다 하니까 우리가 Foundation Model이라고 하는데 그거를 로봇 쪽에도 똑같이 적용을 시키는 거죠. 그러면 generalist, 이런 general한 게 왜 가능해졌냐고 하면 pre-train scale이 되었기 때문이죠. 요즘에 이 Robot Foundation Model은 대부분은 VLA라는 이름으로 불리고 있고요. 이 VLA는 보통 어떻게 만드냐면 VLM을 가지고 만듭니다. LLM이 있고 거기다가 행동을 추가하는 방식으로 만들어요.

그래서 상식이 있고요. 데이터를 최대한 많이 여러 가지 cross-embodiment, 이런 로봇 저런 로봇 다 데이터를 모아서 학습을 시키고 최대한 많은 데이터를 모아서 학습을 시켜서 LLM에서 scaling law가 되었던 것처럼 여기도 모든 task 다 모아서 학습시키면 될 거다, general한 게 될 거다, 이런 가정이 다 기반에 깔려 있고요.

그럼 어디까지 되는데라고 했을 때 Physical Intelligence라는 회사가 잘 보여줬던 것 같아요. π0.5라고 해서 작년 4월에 나온 모델인데 여기서 뭘 보여주냐면요. 영상이 로봇을 싣고 새로운 집에 갑니다. 완전히 새로운 집에 로봇을 갖다 놓고 일을 시키는 거예요. 설거지를 시키는데 우리도 남의 집에 가서 설거지를 하라고 하면 수세미가 어디 있는지 모를 수는 있겠지만 수세미 같은 걸 눈으로 찾죠. 다르게 생긴 수세미도 대충 수세미 같아 보이면 잘 찾아서 알아서 설거지를 합니다. 새로운 환경에 가서도 이걸 다 잘 해야 되거든요. 그래서 그거를 보여준 예시예요. 비슷한 집이 비슷하게 생겼으면 가서 다 할 수 있다. 이 정도 generalization 우리 달성했다, 이런 걸 보여주는 데모였다고 생각을 합니다.

## Physical Intelligence π0.5: 일반화 데모    *17:53*

<span class="paragraph-timestamp" data-ts="18:43">18:43</span> **노정석** 이 π0 만든 Physical Intelligence가 Stanford의 Chelsea Finn 교수님이 창업한 그 회사인 거죠?

<span class="paragraph-timestamp" data-ts="18:51">18:51</span> **박종현** 맞습니다. 아까 여기 옆에서 일을 시키고 있었던 이분이 Chelsea Finn 교수님이거든요.

<span class="paragraph-timestamp" data-ts="18:58">18:58</span> **노정석** 맞아요. 저 랩에서 OpenVLA 만들고 했었던 걸로 저는 또 기억하고 있어서.

<span class="paragraph-timestamp" data-ts="19:07">19:07</span> **박종현** 너무나 많은 걸 해가지고. 모델들도 그렇고 방법론들도 그렇고 어떤 몸체도 그렇고, 여기 창립자분이 학계 분들은 두 분 계신데 가장 유명한 분들이 Stanford, 아마 Berkeley였던 것 같아요. Chelsea Finn이랑 Sergey Levine 이렇게 두 연구자분들이 너무나 이쪽 VLA 쪽에 기여를 한 연구를 많이 하셨죠.

<span class="paragraph-timestamp" data-ts="19:31">19:31</span> **노정석** 쭉쭉 가보시죠. 거의 한 2년 사이에 일어난 변화들이에요. 예전에 그 DeepMind RT-1, RT-2, OpenVLA 나왔을 때만 하더라도 장난감 수준이었는데 말씀하신 딱 이 작년, 작년 동안 어마어마하게 발전했다는 그런 느낌을 가지고 있습니다.

<span class="paragraph-timestamp" data-ts="19:49">19:49</span> **박종현** 맞습니다. 작년에 정말 VLA가 쏟아져 나왔고요. 그래서 조금만 더 헷갈리실 수 있는 제가 용어들을 한 번만 정리를 하고 가자면 제가 RFM Robot Foundation Model, VLA Vision-Language-Action Model, 혹은 LBM Large Behavior Model 이렇게 모델들을 제가 비슷한 용어들을 막 썼는데 얘네들 한 번만 정리를 하고 가겠습니다. 우선 이 물리적인 지능, Physical Intelligence를 만들기 위한 가장 중요한 요소로 꼽히는 게 지금은 VLA예요. VLA는 이름부터 너무 단순한데 저희가 LLM이 있죠.

## VLA 용어 정리: RFM, VLA, LBM    *19:55*

<span class="paragraph-timestamp" data-ts="20:27">20:27</span> **박종현** 제가 일부러 SmolLM이라는 LLM을 하나 가져왔는데 HuggingFace에서 주도적으로 하고 있는 프로젝트죠. LLM에다가 vision encoder를 하나 붙여서 VLM을 만듭니다. 우리가 쓰고 있는 ChatGPT나 이런 서비스들이 다 이미 이미지를 보고 있는데 LLM에다가 vision을 붙여서 눈을 붙여서 만든 거죠. 그래서 SmolLM 같은 경우에는 vision encoder를 붙여서 SmolVLM 이런 걸 만들어서 다 나와 있어요. 거기다가 action을 하나를 더 추가를 하는 거예요. action을 더 추가를 해서 SmolVLA가 됩니다. 이렇게 되면 LLM에다가 양쪽에다가 눈 붙이고 행동 붙여서 만든 모델이 VLA이고 대부분의 VLA는 거의 다 이렇게 만들어지고 있어요.

현재 그래서 이건 SmolVLA는 HuggingFace에서 만든 VLA 예시일 뿐이고 이 친구 같은 경우에는 SmolLM에서 SmolVLM, SmolVLA까지 가는 게 레시피가 다 공개가 되어 있습니다. 그래서 재현 가능한 VLA이고요. 대신에 그렇게 엄청 좋지는 않습니다. frontier 모델이라고 보기는 어렵고 HuggingFace 거이듯이 따라 해 볼 수 있는 그런 모델이다 이렇게 볼 수 있고요.

<span class="paragraph-timestamp" data-ts="21:39">21:39</span> **노정석** action이라는 게 구독자분의 이해를 돕기 위해서 action에 대해서 조금 더 정리를 해보면 로봇이 있을 텐데 우리가 보통 몸체, embodiment라고 하는 그 로봇이 그 형태에 따라서 모터가 달려 있는 데도 다 다르고 손가락 달린 것도 있고, 그런데 그것들이 모터의 어떤 좌표를 줌으로써 실질적으로 어떤 action이 일어나게 되는 건데 그 모터의 좌표들을 뽑아내는 거라고 이해하면 좀 편하려나요?

<span class="paragraph-timestamp" data-ts="22:15">22:15</span> **박종현** 로봇의 각 관절의 각도, 이렇게 표현을 할 수도 있고요. 혹은 손의 좌표, 이렇게 표현을 할 수도 있고 각도라고 표현하면 가장 편할 것 같아요. 사람은 근육을 통해서 움직이지만 어쨌든 로봇은 안에 있는 모터가 돌아가는 거기 때문에 팔꿈치가 몇 도로 펴져 있냐, 이런 것들을 다 action value로서 나타냅니다.

조금만 더 쉬운 예시를 한번 생각을 해보면 게임이라고 생각하면 쉽거든요. 로봇 대신에 게임 캐릭터라고 생각하면 화살표를 누르잖아요. 화살표가 action인 거예요. 앞으로 가라, 옆으로 가라, 팔을 펴라, 이게 다 action인 겁니다.

<span class="paragraph-timestamp" data-ts="10:16">10:16</span> **노정석** 좋습니다.

<span class="paragraph-timestamp" data-ts="22:55">22:55</span> **박종현** 게임 얘기가 나와서 하나만 더 해보자면 VLA를 게임에서도 열심히 쓰려고 합니다. 게임 회사들도 그래서 여기에 엄청 관심이 많고요. 이 VLA를 조금만 더 분류를 해서 정리를 하자면 결국 Robot Foundation Model, 범용적인 로봇을 컨트롤하는 모델을 만들기 위한 수단이고 그 수단이 VLA, LBM 이렇게 자기네들 마음대로 이름을 붙여서 쓰고 있는데 거의 지금은 VLA라는 이름으로 통합이 되어 가고 있는 것 같아요. 혹은 Robot Foundation Model은 지금 소개하지는 않았지만 VLA가 아니어도 구현이 가능하거든요. LLM으로부터 꼭 만들어야 하는 건 아니에요. 다른 방식으로도 우리가 Robot Foundation Model을 구현을 할 수가 있기 때문에 그런 노력도 일각에는 있기는 하다. 일단 지금 여기까지는 VLA이랑 Robot Foundation Model이랑 거의 align이 되어서 칭할 수 있을 것 같습니다.
그래서 이게 잘될 거냐라고 생각을 하면 저는 될 거라고 생각을 하는데 낙관적으로 생각하는 이유가 LLM이 잘 되는 걸 우리가 봤기 때문이거든요. 그대로 하면 이것도 되지 않을까, 간단하게 그렇게 생각을 하는 거예요.

## 핵심 병목: action 데이터는 인터넷에 없다    *24:01*

<span class="paragraph-timestamp" data-ts="24:05">24:05</span> **박종현** 그럼 LLM이 왜 똑똑하냐 이런 걸 봤을 때 제가 생각했을 때 가장 큰 이유는 당연히 scaling입니다. 그중에서도 첫 번째는 pre-train scaling이에요. 인터넷에 있는 모든 글을 다 봐서 지식이 엄청나게 많기 때문에 그 지식을 기반으로 행동을 하는 거죠. 대답을 하고 있는 거죠. 그러면 action도 scaling을 하면 되는 거 아닌가, 모두가 다 지금 그렇게 생각을 하고 있는 것 같아요. 그런데 과연 될까라고 생각하면 잘 안 될 수도 있다, 이렇게 문제점을 꼽을 수도 있어요. 왜냐하면 LLM 같은 경우에는 텍스트라는 데이터, 혹은 이미지까지 포함을 하면 이 vision이라는 문제는 이미지라는 데이터가 인터넷에 널려 있어요. 가져와서 배우면 됩니다. 그러면 최소한 GPT-3, 3.5 이 정도 수준까지 올릴 수 있었다 생각할 수가 있는데 문제는 action이라는 거는 인터넷에 존재하지가 않아요.

그 action이라는 데이터가 실제로 이런 식으로 생겼거든요. 제가 한번 보여드리면, 이게 실제로 로봇이 지금 동작을 하고 있는 action 데이터를 로깅한 겁니다. 그래서 각 카메라가 사람 눈처럼 시야에서 카메라가 눈으로 보고 있고 손목에다가도 카메라를 달아서 이렇게 화면들을 보고 있는데 아래에 있는 게 이게 action 데이터예요. 여기에 이 흘러가는 이 친구들이 action value들이고 이게 각 관절의 각도예요. 팔이 펴졌다가 접혔다가 하는 겁니다. 이런 움직임 데이터가, 이러한 action 데이터가 인터넷에 존재하지 않아서 학습할 게 없는 거죠. 그래서 scaling이 어려워요.

여기가 첫 번째 문제고, 그럼 어떻게 해야 되지라고 하면 가장 간단한 방법은 당연히 teleoperation이에요. teleoperation은 좀 놀라운 예시를 하나 가져왔는데, 저도 이런 게 존재하는지 몰랐었거든요. 이게 1957년이더라고요. 지금보다 한 60년 전이죠. 70년인가요? 한 70년 정도 됐는데, 이 teleoperation이 옛날에도 이렇게나 잘 됐더라고요. teleoperation이라는 거는 이렇게 원격으로 조종을 하는 겁니다. 뒤에서 사람이 어떤 방식으로든지 간에 로봇을 조종해서 동작을 시키는 거고, 이걸 그대로 logging을 하는 거예요. 물론 이때는 logging이 불가능했겠죠. 너무 옛날이니까 컴퓨터나 이런 것들이 잘 안 돼 있었을 테니까. 그래서 이때 로봇의 움직임을 그대로 logging하는 방식이고, 이게 가장 유명한 로봇 중 하나인데 teleoperation 시스템인데, 양팔이 있고 사람이 조종을 하면서 일을 합니다. 지금 이 상황을 그대로 logging을 해서 action 데이터가 저장이 되게 되고, 이걸 학습을 시키는 거예요.

## 데이터 수집의 현실: Teleoperation과 다양한 접근법    *25:55*

<span class="paragraph-timestamp" data-ts="26:58">26:58</span> **박종현** 그러면 이 비슷한 task들은 다 수행을 할 수가 있게 된다. 그래서 이걸 누가 어떻게 하고 있었냐? Tesla, Tesla가 이 training 데이터를 human teleoperation으로 우리가 모으고 있다. 이렇게 VR을 끼고 모으고 있는 거죠. 로봇을 조종하면서 이런 영상이 공개가 되었습니다. 이걸 자랑을 한 거죠. 우리가 이렇게 데이터 공장을 운영을 하고 있다. 현재는 이렇게 안 하고 있다고 합니다. 다른 방법으로 넘어갔는데, 어쨌든 Tesla에서 이 teleoperation 수집을 위해서 고용을 했었는데 한 2년 전 됐던 것 같아요. 2년 정도 전에 시급을 50달러를 주고 고용을 했거든요. 그런데 지원 조건을 보면 키도 로봇이랑 비슷해야 되고요. 하루에 7시간 이상 10kg을 메고 걸을 수 있어야 된다. 진짜 physical한 동작을 시키겠다는 거죠. 근데 이거를 제가 못 할 것 같더라고요. 10kg을 메고 7시간 이상 걷기 쉽지 않을 것 같더라고요.

이런 teleoperation을 제가 저희가 지난 라이브 세션에서 직접 한번 해봤었던 예시인데, 제가 이렇게 Vision Pro를 끼고 이건 실제 로봇은 아니고 시뮬레이터 상에서 제가 컨트롤하는 겁니다. 이런 거를 직접 해보면 좀 어지러울 수 있습니다. VR 세상에서 제가 하고 있는 걸 그냥 화면을 따온 거기 때문에 이런 게 데이터가 되는 거죠. 핸들 옮기기 문제가 있어요. 이걸 해보면 한 2시간 하면 얼굴이 아픕니다. 그리고 한 4시간 하면 멀미가 나요. 장기간 일을 하는 게 너무 어렵거든요. 그리고 이걸 장기간 한다고 하더라도 데이터가 scalable하지가 않아요. 한 사람당 하나, 로봇 하나 사람 하나당 하나. 그래서 데이터를 인터넷 scale, 지금 우리 세상에 있는 인터넷 scale의 텍스트 데이터는 사실 인터넷이 생기고 모든 인간들이 작성한 글들이 다 모여 있는 수준의 scale인데, 이건 scalable하지 않은 거죠. 어쨌든 너무나 teleoperation이 힘들어서 조금이라도 scalable하게 더 만들려고 하는 연구가 이런 연구예요.

잘 안 되다 보니까 이런 걸 UMI라고 하거든요. UMI라는 연구인데, 저런 UI라는 걸 만들어서 저렇게 지금 logging을 하고 있는 거예요. 조금 더 사람이 훨씬 편하게 데이터를 logging할 수 있는 방법이고, 이렇게 action 데이터를 따서 학습을 시킨 모델들이 많이 있습니다. 현재는 다른 방식의 접근들도 있는데, 이게 teleoperation을 하는 거죠. 이건 NVIDIA가 미는 방법 중에 하나입니다. 이렇게 teleoperation은 시뮬레이터 상에서 제가 아까 보여드렸던 것처럼 데이터를 모으고요. 그다음에 뭘 하냐면 뻥튀기를 시킵니다. 시뮬레이션 상에서 비슷하게 로봇을 아무렇게나 randomize해서 움직이도록 만드는 거예요. 그다음에 시뮬레이터 상이니까 막 실패를 해도 괜찮죠. 이 중에서 성공적인 데이터만 모아요. 선별을 합니다. 그걸 가지고 학습을 시키는 거예요.

그다음에 이런 걸 trajectory, 궤적이라고 하는데 로봇의 궤적들을 다변화를 시키는 거고, 그것만 하는 게 아니라 똑같은 움직임이 있었으면 어떤 재질이라든가 배경이라든가 조명이라든가 이런 것들을 바꿔서 다양하게 데이터를 더 만드는 거죠. 그래서 상황을 다양하게 만들어서, 이거는 Cosmos라는 NVIDIA에서 주장하는 world model이라는 모델로 데이터를 뻥튀기를 시키는 거예요. 이렇게 데이터를 쫙 만들어서 scaling을 하겠다라는 게 NVIDIA의 어떤 접근법 중 하나입니다.

## 시뮬레이션 기반 접근: NVIDIA Cosmos와 Sim-to-Real Gap    *29:20*

<span class="paragraph-timestamp" data-ts="30:27">30:27</span> **노정석** 그러니까요. 그 앞은 딱 강화 학습이고, 뒤는 저희 예전에 ImageNet이나 CNN 학습할 때 보던 dataset augmentation이네요. 딱 그러니까요.

그래서 여기 시점에서 dataset 이야기가 딱 나와서, 아마 뭐죠? 아까 LLM과 VLM과 VLA의 정의만 딱 드리고 여기로 넘어왔기 때문에 또 아마 조금 혼동이 있으신 분들도 있을 텐데, 그 action이라는 거에 dataset이 무엇이다라는 거를 종현 님이 아까 그래프로 화면으로 보여주셨고, 카메라가 3개 달린 게 있고 또 그 카메라 3개를 가지고 어떤 목적을 통해서, 그 목적은 보통 텍스트로 들어오겠죠. 근데 그걸 수행하기 위해서 이 나왔던 모터들이 몇 도를 옮겨야 되냐, 저것들의 조합이 어떤 위치인 거잖아요. manipulator 팔 끝에. 이런 것들이 dataset이고, 모델이 이런 데이터를 학습하도록 만들어져 있는 거죠. 마치 transformer가 아래 단어를 넣으면 저 위에서 다음 단어가 튀어나오도록 만든 것처럼, 얘도 어떤 이미지와 텍스트와 이걸 다 집어넣으면 끊임없이 이 action을 내놓을 수 있는 어떤 아키텍처가 있는 거죠.

그거를 종현 님이 VLA라고 정의하신 거고, 그리고 그걸 학습하기 위해서 이 만들어진 dataset이 한번 보여주신 거고, 그다음에 그 dataset이 왜 언어와는 달리 얻기가 어려운지, 그리고 그걸 얻기 위해서 시뮬레이터, 그다음에 직접 하는 teleoperation 이런 형태로 dataset을 취득한다라는 부분들을 이렇게 얘기해 주신 걸로 이해하면 되겠습니다.

그럼 다음 단계로 넘어가 볼까요?

<span class="paragraph-timestamp" data-ts="32:09">32:09</span> **박종현** 이 그림에서 조금만 부연 설명을 하자면, 이렇게 딱 잘라서 vision이랑 language가 input이고 action이 output이 되는 거죠. 그래서 이걸 만들어내는 친구가 VLA인 겁니다.

<span class="paragraph-timestamp" data-ts="32:20">32:20</span> **노정석** task 248로 떠 있겠지만 사실은 텍스트가 안에 있을 거예요. 그렇죠.

<span class="paragraph-timestamp" data-ts="32:22">32:22</span> **박종현** 그렇겠죠.

<span class="paragraph-timestamp" data-ts="32:24">32:24</span> **노정석** 옷을 이렇게 개서 어떤 모양으로 접어라라는 action, 그게 있을 겁니다. 목적이. 제일 중요한 문제 사실 지적해 주신 거거든요.

그리고 종현 님이 말씀하신 dataset을 생성하는 게 scalable하지 않다가 이 시장의 지금 가장 큰 문제이자 기회이자, 혹은 NVIDIA 같은 크기의 대기업이 아니더라도 조그마한 크기의 스타트업들도 지금 기회를 많이 발굴하고 있는 그 영역이라고 보시면 맞을 것 같아요.

<span class="paragraph-timestamp" data-ts="32:55">32:55</span> **박종현** 그래서 제가 여기다 모든 방법을 다 써놓지 않았는데, 예를 들면 Meta 같은 경우에는 안경 같은 게 있거든요. 안경에 카메라를 다 달아놓고 그걸 쓰고 logging하고 행동을 하라고 그런 제품들을 내놓습니다. 그러면 그 안경이 데이터 수집 장치인 거죠. 일단 human이 어떻게 action을 하는지, 그 안경 같은 경우에는 손의 위치라든가 좌표라든가, 손가락까지는 못 따더라도 그런 걸 최대한 다 자동으로 따는 그런 머신이 있거든요. 그래서 안경 형태가 될 수도 있고요. 아니면 Vision Pro 같은 형태가 될 수도 있고, 여러 가지 방식으로 데이터를 따려고 하고요.

제가 기대하는 가장 큰 방식 중 하나는 사실 로봇이 일단 팔리는 겁니다. 로봇이 팔려서 데이터가 돌아다니면 그것들이 데이터가 되는 거죠.

<span class="paragraph-timestamp" data-ts="33:45">33:45</span> **노정석** 어떤 하나의 form factor가 굉장히 싼 거. 사실은 종현 님이 하셨던 HuggingFace의 LeRobot이나 이런 것들도 이런 initiative들과 연결돼 있는 거잖아요. 어떤 딱 정해진 표준 형태의 form factor가 팔리고, 밖에 나가서 dataset도 좀 open domain에서 많아지고, 그런 것들이 또 커뮤니티에서 많은 시도들이 있을 것 같다는 생각은 드네요.

<span class="paragraph-timestamp" data-ts="34:09">34:09</span> **박종현** 맞습니다. HuggingFace는 회사라기보다는 커뮤니티 회사이긴 하죠. 뭐 애매한데, 어쨌든 커뮤니티를 지향하는 곳이기 때문에 로봇도 오픈소스로 만들고 다 오픈소스로, 하드웨어고 소프트웨어고 다 만들어서 튜토리얼도 다 만들어주고 행사도 열고 하면서 로봇을 최대한 뿌려서 그 데이터가 모두 HuggingFace에 올라오도록 만들었고요. 그렇게 해서 모은 데이터로, 사람들이 알아서 자기네들이 공부하면서 올려놓은 커뮤니티 데이터로 만들어진 모델이 SmolVLA입니다. 그래서 SmolVLA의 paper에 가면 기업 repo처럼 HuggingFace의 data repo가 쫙 다 써 있습니다. 이 데이터 가지고 만들었다, 너들 거 가지고 만들었다. 이렇게.

그다음에 얘기 나온 김에 하나만 더 하자면요. 제가 띄워놓았나? 제가 가장 기대하고 있는 게 flywheel인데, 이건 제가 다른 데서 쓰던 거거든요. 이 1X라는 회사의 NEO라는 로봇이 있거든요. 이런 휴머노이드, 여기도 보면 teleoperation을 하고 있죠? 이런 건 못 하고요. 이 회사가 되게 공격적으로 마케팅을 해요. 이것도 지금 iShowSpeed라는 거의 아마 몇 천만, 1억 가까이 되는 유튜버일 텐데, 스트리머일 텐데 이 NEO라는 로봇이 MrBeast 채널에도 나오더라고요. 나와가지고 야구 인간과 대결 막 이런 거 하고 있던데, 어쨌든 홍보를 열심히 해요. 이 로봇을 지금 팝니다. 지금 예약 구매를 작년에 받았어요. 그래서 저도 주문을 해놨는데, 근데 이 로봇이 지금 당연히 VLA가 완벽하지 않으니까 가사 일을 다 하기가 어렵잖아요. 그래서 어떻게 해 주겠다고 하냐면 Tesla처럼 우리가 teleop으로 가사 일은 해줄게, 나중에 잘될 거야. 일단 광고는 빨래 개는 건 다 잘 된다고 하지만, 안 되는 건 teleop으로 해줄게. 이렇게 얘기를 하거든요.

<span class="paragraph-timestamp" data-ts="35:59">35:59</span> **노정석** 그거 굉장히 좋은 비즈니스 모델이네요. 일단은 하드웨어를 풀어서 넣고, 대신 소프트웨어가 아직 준비가 안 된 거니 소프트웨어는 우리가 원격으로 사람이 할게. 대신 거기서 얻어진 걸 가지고, customer는 사실은 어떤 문제가 해결되는 경험을 하는 거고 그다음에 회사는 dataset을 얻는 거고, 그건 양쪽으로 win-win으로 데이터 획득도 되고 현실에서 문제도, 당장 비즈니스적인 효용도 있고. 되게 좋은 접근이네요.

<span class="paragraph-timestamp" data-ts="36:22">36:22</span> **박종현** 저 로봇이 진짜로 예정은 올해 배포가 될 텐데, 배포가 되어서 가정에서 일을 하기 시작하면 데이터가 scalable하게, 꽤나 scalable하게 쌓이는 창구가 될 수 있겠다라고 기대를 하고 있고요. 저도 그래서 바로 일단 order를 넣었고.

<span class="paragraph-timestamp" data-ts="36:38">36:38</span> **노정석** 좋네요. 사실 Tesla도 이런 전략을 정확히 썼잖아요. 일단은 완전 완성되지 않은 FSD를 팔고, 굉장히 기본적인 기능 Autopilot부터 해서 Autopilot을 진전하면서, 고속도로만 됩니다. 그다음에 자동차 전용도로 됩니다. 그러면서 시내 주행, 그다음에 시골길 주행, 이런 걸로 이렇게 펼쳐간 거니까. 결국은 종현 님이 처음 말씀하신 이 original dataset의 coverage 문제랑도 또 정확하게 align된 문제로 들리네요.

<span class="paragraph-timestamp" data-ts="37:08">37:08</span> **박종현** 최대한 다양한 환경에서.

근데 좋은 비즈니스 모델이라고 생각이 되는 게 인건비가 사실 나라마다 엄청 많이 다르기 때문에 teleop은 인건비가 되게 낮은 나라에서 해도 되거든요. 그러면 인건비가 높은 나라의 로봇이 배포가 되고, 거기에 가정부 일을 사실 엄청 로봇 몸체라는 대신할 몸체만 있으면 싼 나라의 인건비, 전 세계적으로 어쩌면 physical한 노동이라는 게 가격이 맞춰지는 계기가 될 수도 있겠다라는 생각이 들기도 합니다.

<span class="paragraph-timestamp" data-ts="37:40">37:40</span> **노정석** 그런 게 또 연구가 아닌 비즈니스가 주는 즐거움이기도 하죠. 재밌네요.

<span class="paragraph-timestamp" data-ts="37:43">37:43</span> **박종현** 정리를 하자면, data scaling이 너무 어려워서 이거를 모으기 위한 노력들을 많은 회사들이 다양한 방법으로 다 하고 있다. 정답이 어떻게 될지는 모른다라는 거고요.

일단 제가 기대하는 거는 저 시뮬레이션, NVIDIA가 주장하는 저 시뮬레이션이. 크게는 NVIDIA는 Isaac Sim이라는 물리 시뮬레이터가 있고, 그다음에 world model을 하면서 물리 시뮬레이터가 아니라 진짜 그것도 영상 생성 모델을 기반으로 하는 시뮬레이터가 되는 거죠. 시뮬레이션이 완벽하게 되면 사실 가상으로 데이터를 다 만들 수 있는 거기 때문에 action scaling 문제라는 게 풀릴 거예요.

근데 지금으로서는 시뮬레이션과 real의 갭이 있기 때문에 이 sim-to-real gap이라고 하거든요. 이 갭을 메꾸기가 아직은 너무 어려워서 NVIDIA도 데이터를 섞어서 씁니다. 텔레오퍼레이션한 데이터, 실제 데이터랑 가상 시뮬레이션 데이터랑 뻥튀기시킨 데이터랑 이런 것들을 모아서 섞어서 학습을 하고 있는데 시뮬레이션이 정교해지면 갑자기 풀릴 수도 있는 문제다, 이렇게 저는 생각을 하고 있습니다.

## Scaling Law와 타임라인 전망    *38:46*

<span class="paragraph-timestamp" data-ts="38:47">38:47</span> **최승준** 이 분야에도 scaling law 비슷한 게 있나요? 그게 하나 질문이고, 두 번째는 스케일링이 작동한다고 했을 때는 늘 창발적인 현상 같은 것들이 있어 왔는데 다른 영역에서는 비슷한 게 있었는지가 궁금해지는 부분이네요.

<span class="paragraph-timestamp" data-ts="39:01">39:01</span> **박종현** scaling law라는 게 적용될까에 대해서, 이거는 제가 의견만 작성하고 안에 내용은 다 Claude가 조사를 한 건데 이런 내용, 연구랑 내용이 있습니다. 예를 들어서 Generalist라는 또 유명하신 분들이 나와서 만든 회사가 있거든요. 여기서 데이터 스케일링을 UMI 스타일로 엄청 크게 해서 자기네들이 모아가지고 최대한 모아가지고 해봤더니 full 데이터, 그러니까 텔레오퍼레이션 데이터가 많아지면 많아질수록 훨씬 잘 된다, 이런 똑같은 law를 숫자적으로 증명을 했어요.

근데 여기서의 스케일링은 당연히 LLM만큼의 데이터가 존재하지 않기 때문에 그만큼의 스케일링은 아니지만 우리가 어쨌든 모으면 모으는 대로 좋아지더라라는 걸 똑같이 관찰을 한 것 같고요. NVIDIA GR00T에도 가짜 데이터를 최대한 많이 모아서 넣었더니 얼마나 더 좋아지더라, Physical Intelligence에도 비슷한 연구들 같은 게 다 있습니다.

지금 상당히 초기 단계지만 거의 모든 VLA를 만드는 곳에서 비슷한 이야기를 하고 있어요. 데이터가 많아지면 잘 되는 건 확실하다. 그런데 어디까지 갈지는 모르겠다.

<span class="paragraph-timestamp" data-ts="07:56">07:56</span> **노정석** 똑같은 거죠.

<span class="paragraph-timestamp" data-ts="40:17">40:17</span> **박종현** 이런 건 해봐야 아는 거고요. 그다음에 그런 emergence가 관찰이 되었느냐 하면 제가 알기로는 딱히 그런 엄청 특별한 것들은 아직은 없다고 알고 있습니다. 지금 현재로서는 in-distribution, 그러니까 우리가 학습시켰던 케이스는 무조건 되는 건 거의 확실하고 out-of-distribution, 새로운 환경에 가서도 잘될 것이냐, 이게 궁금한 건데 일부 된다까지 나온 것 같고요. 이게 스케일이 더 커지면 말씀하신 것처럼 그런 emergent한 창발적인 무언가가 될까 하면 저는 낙관적으로 생각을 하고 있는데요. 왜냐하면 LLM 됐으니까 얘도 되겠죠. 그리고 인간도 되니까 되겠죠. 이 정도로만 생각하고 있습니다.

<span class="paragraph-timestamp" data-ts="41:03">41:03</span> **노정석** 저도 명확하게 동의합니다. 저희 LLM도 처음엔 그랬잖아요. 이 문제 못 풀 거예요, 이 문제 못 풀 거야 하는 걸 끊임없이 깨는 그런 연속이었고 벤치마크만 주기만 해, 우리가 다 그냥 one policy에서 전부 다 만들어주겠어라고 하는 단계에 온 거니까. 이 robot foundation model, 이런 부분들이 굳이 표현을 하자면 GPT-2 정도, 그 단계라고 보면 얼추 비슷한 느낌이지 않을까라는 생각이 들어요.

<span class="paragraph-timestamp" data-ts="41:31">41:31</span> **박종현** 그렇죠. 첫 번째 generality를 보이기 시작한 단계, 이렇게 볼 수 있겠죠.

<span class="paragraph-timestamp" data-ts="41:39">41:39</span> **최승준** 그러니까 이게 언제이냐의 타이밍 문제인 거잖아요. 사실 분위기가 어떻게 얼마큼 형성됐고 그런 얘기인 거잖아요. 느낌적인 느낌으로.

<span class="paragraph-timestamp" data-ts="41:47">41:47</span> **노정석** 근데 시장의 인센티브가 LLM 쪽은 지금 좀 뭔가 끝났다, 대형 회사들이 이미 끝났다라는 이런 인식들이 지배하고 있기 때문에 그리고 투자해야 되는 비용도 매우 크고. 이 Physical AI 쪽으로 종현님도 좋은 예인데 굉장히 똑똑한 사람들이 이쪽으로 어마어마하게 투입되고 있습니다. 그러면 여기도 자본과 talent가 만나 있으니까 시간 문제인데 점점 더 가속하고 있는 느낌이 들어요.

<span class="paragraph-timestamp" data-ts="42:14">42:14</span> **최승준** 그러니까 GPT-3 나올 것 같은 즈음인 거죠.

<span class="paragraph-timestamp" data-ts="42:20">42:20</span> **노정석** GPT-3 이제 올해 금방 나올 거고 사실 ChatGPT 모멘트와 아까 종현님이 말씀하셨던 여기도 뭔가 foundation model이라고 부를 수 있는 포인트 올해 안에 맞지 않을까 싶어요. 종현님의 타임 예측은 어때요? 올해 여름 정도면 여기도 막 다 박수 치고 있고 그럴 분위기이긴 해요. 제 느낌에.

<span class="paragraph-timestamp" data-ts="42:39">42:39</span> **박종현** GPT-3 모멘트를 어느 정도로 보느냐에 따라 다를 것 같은데요. 결국 진짜 유저들이 쓸 수 있을 정도, 이렇게 정의를 한다면 저는 올해일 거라고 생각을 합니다.

<span class="paragraph-timestamp" data-ts="36:22">36:22</span> **노정석** 올해.

<span class="paragraph-timestamp" data-ts="42:57">42:57</span> **박종현** 늦어도 내년일 거라고 생각을 합니다. 진짜로 로봇이 배포되어서 특정한 task, 꽤나 general한 task들을 시장에서 가져오기 시작한 순간. 올해 아니면 내년일 거라고 저도 생각을 합니다.

<span class="paragraph-timestamp" data-ts="43:06">43:06</span> **노정석** 제가 실제 회사들에서는 어떻게 하고 있는지에 대해서 정확한 데이터는 없어서. 대부분의 회사들도 예를 들어 LLM에서도 그렇잖아요. 저희 아키텍처 자체는 몇 가지로 통일되고 Transformer에서도 약간의 변종들이 끊임없이 생겨나는 것처럼 여기도 지금 VLA가 예전 RT-1, RT-2, VLA에서 Chelsea Finn 교수님의 π0, 거기가 만든 변화, 그다음에 SmolVLA가 보여준 변화, 그다음에 NVIDIA GR00T도 오픈소스로 내놨다라고 보는데 어떠한 하드웨어를 쓰느냐에 따라서 작은 하드웨어면 오픈소스를 쓸 것 같고 큰 하드웨어면 더 큰 모델을 쓸 것 같고.

모델과 이 하드웨어가 정해지고 나면 이거는 데이터셋 생성하면 웬만하면 다 되는 문제로 지금 되고 있나요?

알고리즘과 데이터셋 취득 사이에 어떤 비율, 노력이 투입되는 정도, 어느 정도로 저희가 받아들이면 대충 맞을까요?

## VLA 모델의 수렴 진화와 남은 논쟁    *44:04*

<span class="paragraph-timestamp" data-ts="44:05">44:05</span> **박종현** 제가 숫자로 표현하긴 어려울 것 같고요.

<span class="paragraph-timestamp" data-ts="44:07">44:07</span> **노정석** 감으로 말씀 주세요.

<span class="paragraph-timestamp" data-ts="44:14">44:14</span> **박종현** 알고리즘이라기보다는요, 모델은 어느 정도 수렴을 한 것 같고요. 모델의 구조, VLA는 대충 이런 식으로 만들면 되더라. 그런데 다른 논의들, 아직 해결되지 않은 논의들이 있는 것 같아요. 예를 들면 촉각이라는 게 필요할까, 혹은 손가락이라는 게 필요할까, 그냥 집게로도 되는 건가, 다섯 손가락이 꼭 있어야 하는가, 아니면 또 뭐가 있을까요?

지금 LLM을 통해서 올라온 이 VLM, VLA, 이렇게 만들어진 구조 자체가 진짜로 한계가 아예 없을까. 이렇게 훨씬 다른 차원의 문제를 풀고 있는 것 같거든요.

그러니까 당장의 task들, 이렇게 구분하면 될 것 같아요. 텔레오퍼레이션으로 우리가 할 수 있는 task는 진짜 데이터만 모으면 무조건 되는 것 같아요.

근데 텔레오퍼레이션으로 안 되는 task들이 있거든요. 저도 제가 실제로 동작을 해보면 예를 들어서 제가 텔레옵으로 제가 하나만 빠르게 한번 보여드리겠습니다. 이게 Physical Intelligence의 주장이기도 합니다. 텔레옵으로 혹은 다섯 손가락이라는 게 필요하냐, 다섯 손가락이 상당히 어렵거든요.

<span class="paragraph-timestamp" data-ts="45:28">45:28</span> **노정석** 그래서 회사들마다 복잡한 하드웨어 폼팩터를 두고 복잡한 문제를 비즈니스 포지션으로 삼고 있는 경우들을 저희가 많이 보게 되잖아요. 그래서 우리는 다섯 손가락 폼팩터의 로봇과 특정 도메인의 문제를 결합한 이 문제를 해결하는 회사다라는 게 있고

결국은 근데 그게 다섯 손가락이 아닌 손가락 2개나 아니면 전통적인 그냥 gripper로 해결할 수 있는 문제일 수도 있는 거고 그런 차이에 따라서 다 달라지는 거잖아요.

<span class="paragraph-timestamp" data-ts="46:01">46:01</span> **박종현** 제가 직접 한번 해본 건데 저의 챌린지거든요. 기어를 조립하는 거예요. 텔레오퍼레이션으로 기어를 이렇게 조립하는 걸 제가 직접 텔레오퍼레이션 구현을 해서 해보고 있는 건데 진짜 안 되더라고요. 왜 안 될까 생각을 해보니까 저게 구멍이 너무 작아요. 그러니까 딱 껴지는 그런 조립이에요. 기계 조립, 제가 이게 촉각이 없으니까 안 되더라고요.

<span class="paragraph-timestamp" data-ts="34:23">34:23</span> **노정석** 리얼?

<span class="paragraph-timestamp" data-ts="46:21">46:21</span> **박종현** 아니요. 저거는 시뮬레이터.

<span class="paragraph-timestamp" data-ts="46:22">46:22</span> **노정석** 시뮬레이터죠.

<span class="paragraph-timestamp" data-ts="46:26">46:26</span> **박종현** 제가 촉각이 없으니까 진짜 잘 안 돼요. 이런 task를 우리가, 촉각이 필요한 작업이 생각보다 우리 세상에 많더라고요. 제가 직접 하고 있는, 인간들이 하는 행위들 중에. 그래서 촉각이라는 게 필요할까 아닐까, 이런 문제로 넘어가게 되면 거기에 집중을 하는 회사들이 있는 것 같아요. 회사들마다 완전히 방향이 다른데 첫 번째는 이 촉각이 없어도 되는 일이 엄청 많거든요. 설거지 같은 건 촉각 없어도 됩니다. 그런데 이러한 촉각이 필요한 작업을 나는 풀겠다라고 하는 곳들은 촉각을 그럼 어떻게 해야 되지, 센서는 어떻게 생겨야 되지, 이런 문제들을 풀고 있는 것 같아요.

그래서 다시 돌아와서 좀 길어졌는데 회사들마다 어떤 식의 연구를 하느냐라고 하면 이렇게 다들 niche한 문제들을 다 각자 정의를 다르게 하고 있는 것 같아요. 정말 모든 인간이 하는 노동을 다 하겠다라고 하는 곳들은 특히 학계가 그런 접근을 하죠. 촉각에 많은 연구들을 하는 것 같아요. 일례로 학회 같은 데 가면 한 절반 정도는 촉각에 대한 이야기를 합니다. 그런데 업계 같은 경우에는 그런 데 집중하는 곳들이 그렇게 많지는 않은 것 같고요. 데이터를 스케일링하는 데, 일단 당장 돈을 벌 수 있는 그런 곳에 집중을 많이 하고 있는 것 같아요. 스타트업들은 대부분 데이터에 다 집중을 하고 있는 것 같고 하드웨어 업체들은 더 정교한 손, 이런 것들을 만드는 데 노력을 하고 있는 것 같고 학계는 촉각이나 혹은 RL, 이곳에서도 reinforcement learning을 어떻게 할 수 있을까 이런 연구들을 하고 있는 것 같다, 이렇게 정리할 수 있을 것 같아요.

그 질문 주신 내용들을 하려고 했는데요. 이 모델들을 한번 보려고 해요. 실제로 그런 VLA들이 이제 여기에 대한 연구를 얼마나 하고 있느냐, 이거는 작년에 나온 모델들을 저희가 한번 보면 좀 정리가 잘될 것 같은데 제가 사실 최신순으로 정리를 해가지고 역순으로 정리를 했거든요. 아까 말씀하신 게 이 RT, RT죠. 구글에서 열심히 했던 건데 Robotics Transformer, 그러니까 Transformer를 action을 출력하도록 만든 거죠. 이때는 결국 언어에서 출발한 거기 때문에 행동을 언어처럼 생각을 한 거예요. 똑같이 토큰들이 나오고 그 토큰이 하나하나의 행동으로 매핑이 되고 그러면 이런 일들을 할 수 있다. RT 시리즈들이 막 나왔었는데 2024년, 재작년이죠. 재작년에 OpenVLA라는 게 나왔고요. 이건 오픈 진영에서 이 정도 되는구나라는 걸 첫 번째 보여줬던 것 같고 제가 생각했을 때는 이 VLA의 시작, 이렇게 관심을 많이 가지게 된 정확히는 연구는 미리 하고 있었겠지만 대중들이 관심을 가지게 된 계기는 π0였던 것 같아요.

Physical Intelligence라는 회사의 π0가 딱 나오면서 아 이게 되네, 이런 걸 처음 깨달았던 것 같고 이 π0가 결국엔 작년에 π0.5, π\*0.6 이렇게 나왔거든요. 그래서 이 모델들, 작년에 나온 모델들을 보면 이렇게 모델들이 막 나왔습니다. Figure에서 아까 봤었던 Figure에서 Helix가 나왔고 NVIDIA에서 GR00T라는 게 나왔고 Google에서 Gemini Robotics가 나왔고 HuggingFace에서도 이런 거 내고 Boston Dynamics에서도 LBM, Toyota Research랑 해서 같이 내고 이런 모델들이 쭉 나왔는데 이거 말고도 사실 엄청 많이 나왔습니다.
나왔는데 얘네들의 수렴된 포인트를 보면 2025년, 제가 이렇게 꼽았거든요. 이건 사실 저희 공동 호스트인 J씨가 이런 의견을 가지고 있어서 저도 동의를 한 부분인데 수렴 진화를 했다.

## VLA 계보: System 1/2 구조    *48:04*

<span class="paragraph-timestamp" data-ts="50:08">50:08</span> **박종현** 그래서 거의 모델들이 딱 까서 보면 다 비슷하게 생겼어요. 첫 번째 포인트는 System 1, 2 구조를 가지고 있어요. 이게 승준님께서 예전에 이 얘기하셨던 것 같은데 Kahneman, Thinking, Fast and Slow. 이런 어쨌든 인간의 지능이라는 게 어떻게 생겼는가 그런 것들을 차용을 해서 모델 구조에 녹아들어가 있는데요. 한번 볼게요.

GR00T N1.6, 이게 가을쯤에 나온 버전입니다. 보면 System 1, 2 구조가 되어 있어요. VLM이 있고요. Diffusion Transformer가 있어서 두 개가 조합되어 있는 형태예요. 이 VLM이 결국엔 저희가 알고 있는 그냥 유명한 VLM입니다. 얘는 버전마다 내용이 조금 달라서 어쨌든 이 친구 같은 경우에는 vision input으로 받고요. language input으로 받아서 output이 나갑니다. 저희가 알고 있는 VLM 똑같아요. GPT랑 똑같이 생겼어요. 근데 여기 output을 tokenize할 수도 있고 토큰으로 만들 수도 있고 토큰을 안 하고 그냥 만들기 전에 벡터의 형태로 나갈 수도 있는데 그렇게 output을 내보내고 마지막에 Diffusion Transformer를 하나 붙여서 여기에 이 해석된 결과, 나는 지금 무슨 일을 해야 되고 지금 내가 눈으로 보는 환경은 어떻게 생겼고 이런 걸 이해한 결과와 robot state, 내 몸체, 나의 몸은 지금 현재 어떤 상태인지, 이걸 input으로 받아서 action 토큰이 나와요.

<span class="paragraph-timestamp" data-ts="51:31">51:31</span> **노정석** 저 위에 헤르츠라고 얘기하신 거는 이 두 개의 System 1과 System 2에 System 2가 토큰을 한 번 뱉을 때 System 1은 토큰을 한 몇십 배 더 뱉는다, 이렇게 해석해야 되는 거죠. 주파수가 다르다는 얘기가.

<span class="paragraph-timestamp" data-ts="51:49">51:49</span> **박종현** 상황 인지는 천천히 해도 돼요. 10초에 한 번만 하면 되는데 예를 들어서 행동이라는 거는 엄청 빨라야 되거든요. 반응이 빨라야 되고 그래야 균형도 잡을 수 있고 놓치지 않을 수 있고 많은 것들을 할 수가 있어요. action이라는 건 훨씬 빨라야 되기 때문에 이렇게 나눈 거죠.

<span class="paragraph-timestamp" data-ts="52:01">52:01</span> **최승준** 흥미롭네요. System 2가 먼저네요.

<span class="paragraph-timestamp" data-ts="52:05">52:05</span> **박종현** 여기는 이렇게 붙이더라고요. 어쨌든 숫자가 1, 2 이렇게 붙여져 있지만 큰 인지를 하는 거랑 빠르게 반응해야 되는 action이라는 거는 나눠야 된다는 거.

<span class="paragraph-timestamp" data-ts="52:14">52:14</span> **노정석** 저희가 LLM에서 얘기하는 System 1, 2랑은 조금 다른 개념인 것 같긴 해요.

<span class="paragraph-timestamp" data-ts="52:21">52:21</span> **최승준** 직관이 원래 System 1 쪽이니까 빠른 거가 System 1이라서 이렇게 되어 있는 거고. Diffusion Transformer가 쓰인다는 건 결국에는 action이 생성된다는 의미인 거죠.

<span class="paragraph-timestamp" data-ts="52:29">52:29</span> **박종현** Diffusion Transformer가 action value를 만들어 내는 거죠.

<span class="paragraph-timestamp" data-ts="52:32">52:32</span> **최승준** 다양하게 다양하게 생성되는 그런 느낌.

<span class="paragraph-timestamp" data-ts="52:35">52:35</span> **박종현** 그다음에 Figure Helix도 보면

<span class="paragraph-timestamp" data-ts="52:37">52:37</span> **노정석** 저 denoising이라고 돼 있는 부분이 저게 VLA랑 다른 거죠. OpenVLA는 그냥 하나의 Transformer 모델에서 토큰이 막 튀어나왔던 거고 얘는 아예 그 action을 따로 분리해서 그 action 부분은 Diffusion 모델로 action을 생성하는 형태로 바뀌어 있는 거죠. 맞죠? 그림 잘 그려졌네요.

<span class="paragraph-timestamp" data-ts="53:03">53:03</span> **박종현** 그다음에 Helix도 똑같거든요. Figure Helix도 System 1, 2 구조여서 거의 완전히 똑같아요. 근데 차이점이 있다면 얘는 System 2에서도 robot state를 받는다는 거, 이런 디테일적인 차이들이 있지만 사실 크게 중요한 것 같지는 않고요. 어쨌든 얘도 큰 모델이 느리게 상황 인지랑 명령을 받아서 생각을 하고 그 인지된 벡터를 받아서 action을 만들어야 되는 친구가 빠르게 얘는 200Hz로 action을 만든다.

Gemini Robotics도 마찬가지예요. Gemini Robotics는 심지어 위아래를 여기도 System 1, 2로 구분했는데 상황 인지하는 모델은 클라우드에서 돌리고 클라우드에서 그냥 Gemini가 돌면서 상황 인지랑, vision으로 상황 보고 말도 하고 명령도 받고 생각도 reasoning도 하고 코드도 쓰고 이런 걸 다 한다고 해요. 할 수 있는 거는 다 하고 그다음에 해석된 결과를 어쨌든 action을 내뱉어야 되는 작은 모델한테 보내서 그 모델은 로컬에서 도는 거죠.

그래서 Google은 이거 클라우드에서 저 큰 게 돌아가는 거를 어쨌든 사업 모델로 판매를 하려고 한다는 생각이 들고요. 분리해서 느린 거는 클라우드에서 돌리는 게 대신에 큰 GPU를 쓸 수가 있으니까 서버급의 GPU를 쓸 수가 있으니까 똑똑하게 할 수 있어서 저는 좋은 방법이라고 생각합니다.

<span class="paragraph-timestamp" data-ts="54:21">54:21</span> **노정석** 아까 보여주신 이 지금 보여주시는 모델들이 밖에 오픈 도메인에 완전히 그냥 코드베이스가 나와 있는 건 NVIDIA GR00T밖에 없는 거죠. π 모델이나 SmolVLA은 당연히 오픈 모델일 것 같은데 걔는 뭔가 좀 뭐라고 그래야 될까요? 복잡도가 떨어지는 모델이라는 인식이 들고 NVIDIA는 휴머노이드 대응이라고 하니까 굉장히 많은 대응이 돼 있는 모델이라는 생각이 드는데 그렇게 이해하면 맞나요? 아니면 너무 무식한 이해일까요?

<span class="paragraph-timestamp" data-ts="54:52">54:52</span> **박종현** 조금 타겟이 다른 것 같아요. 일단 제가 여기에 3개 이렇게 소개시켜 드린 거는 System 1, 2 구조라서 3개를 꼽은 거고요. NVIDIA GR00T는 완전히 오픈소스인데 그렇게 큰 모델은 아니에요. 한 3B, 7B 이 정도 되는 모델이고 완전히 오픈이라서 저희가 다 잘 쓸 수 있고요. 그런 장점이 있는데 π 같은 경우에는 closed 모델인데 오픈 버전이 있습니다. 오픈소스로 내놓은 게 있고 모든 게 다 오픈되어 있진 않아요. 어쨌든 저희가 가져다 쓸 수 있을 정도로는 오픈이 되어 있습니다.

다음, 우선 System 1, 2 구조라는 게 왜 필요한가라고 하면 오늘 시작이자 결론에 해당할 이야기인데 intelligence라는 게 사실은 두 개가 어느 정도 분리가 되어 있다는 거를 나타내기도 하는 것 같아요. 상황을 판단하고 고민하고 이런 cognitive intelligence랑 진짜 본능적으로 반응해야 되는 physical intelligence가 사람 뇌에서도 나눠져 있지 않은가. 이런 방식으로 구현을 하는 게 만약에 Robot Foundation Model로서 가장 효율적인 구조라는 게 우리가 알게 된다면 실제 우리 뇌도 그렇지 않을까 오히려 역으로 알게 되는 그런 계기가 될 수도 있을 것 같다는 생각이 듭니다.

<span class="paragraph-timestamp" data-ts="55:57">55:57</span> **노정석** 완전 동의합니다.

## Continuous Action과 Diffusion 기반 접근    *55:59*

<span class="paragraph-timestamp" data-ts="56:00">56:00</span> **박종현** 다음 수렴 포인트는 continuous한 action을 내뱉는다는 건데요. 이게 아까 질문 주신 거랑 이어져 가는 것 같아요. RT나 OpenVLA 이런 친구들은 action value가 discrete하거든요. 왜냐하면 LLM이 원래 Transformer고 LLM이고 얘네들이 다 output 토큰이 다 autoregressive하고 discrete해요. 단어가 다 연속적이지 않아요.

그런데 우리가 이미지도 생각을 해보면 이미지도 그래서 저희가 Diffusion으로 많이 만들잖아요. 이미지도 continuous하니까. 그런 것처럼 action도 사실 continuous하단 말이죠. 예를 들어서 "안녕"과 "헬로"에는 이런 두 토큰 사이에는 가운데가 없어요. 0.1 안녕, 0.9 헬로라는 value가 없습니다. 토큰이 discrete하니까.

그런데 action이라는 거는 continuous하니까 중간이라는 게 다 존재해야 돼서 그래서 이 RT-1이나 이런 친구들의 동작을 한번 보면 행동이 상당히 뚝뚝뚝 끊겨요. 그래서 연속적으로 반응해야 되는 그런 것들이 잘 안 됩니다.

그러다 보니까 action이 continuous해야 되니까 이걸 어떻게 할까 해서 수많은 모델들이 이런 Diffusion을 참고를 하게 됩니다. 지금 여기 나와 있는 예시는 이 Diffusion Policy라는 이건 Transformer 기반은 아니고 그냥 진짜 Diffusion 하나 가지고 action을 만든 모델이에요. 이것도 되게 sensational하게 잘 되는구나 이런 걸 좀 보여줬던 초기 연구 중 하나인데 이제 섞기 시작한 거죠. Diffusion도 Transformer에다가 붙이기 시작한 거죠.

System 1, 2를 붙였듯이 continuous하고 빨리빨리 만들어내고 이런 것들이 가능한데 denoising을 이건 Diffusion에 대해서 조금 알아야 되는데 denoising을 쫘쫘쫘작 해야 되기 때문에 연산하는 게 조금 다릅니다. 방식이.

<span class="paragraph-timestamp" data-ts="57:38">57:38</span> **최승준** denoising을 홀로 다 하는 건 아니죠.

<span class="paragraph-timestamp" data-ts="57:41">57:41</span> **박종현** 이 Diffusion Policy는 홀로 다 하는 겁니다.

<span class="paragraph-timestamp" data-ts="57:44">57:44</span> **최승준** 풀로 다 하는 거예요?

<span class="paragraph-timestamp" data-ts="57:47">57:47</span> **박종현** 이 연구는 그랬고요. 요즘에 VLA는 섞여서 하는 거죠.

<span class="paragraph-timestamp" data-ts="57:52">57:52</span> **노정석** 이게 한 번 Diffusion이 돌 때마다 여러 개의 action step을 보여주는 건데 그 중간 state에 갈 때도 끊임없이 state가 계속 바뀌네요. 그냥 여러 state, action state들이 끊임없이 중첩된 형태로 막 튀어나오는 그런 걸 동영상이 보여주려고 하는 것 같네요.

<span class="paragraph-timestamp" data-ts="58:12">58:12</span> **박종현** 미래의 action들을 쫘쫘쫘작 내뱉었는데 그 행동을 다 하기 전에 Diffusion이 또 돌죠. 그걸 말씀하시는 거죠. 되는 만큼 되는 거예요. 왜냐하면 행동을 미래를 예상해서 했는데 막상 행동을 진짜 하고 나면 상호작용이 바뀔 수가 있잖아요. 그러면 그 관찰값에 따라서 다시 action을 generate해야 되는 거죠. 사람도 그렇게 하죠. 균형 잡는 거라든가 이런 게 다 그런 거죠. 그래서 되는 만큼 계속 빨리 하는 게 당연히 더 좋습니다.

그래서 π0 같은 경우에도 이렇게 생겼거든요. pre-trained VLM, 이게 아마 PaliGemma를 썼던 것 같아요. 이것도 버전마다 모델이 조금 다 달라가지고 그냥 VLM, 저희가 알고 있는 그냥 많이 쓰는 VLM 가져다가 거기 뒤에다가 여기는 action expert라고 이름을 붙였는데 여기는 Flow Matching이라는 또 Diffusion이랑 비슷한 그런 알고리즘을 사용을 합니다. 두 개를 붙여서 input이 들어가면 명령이랑 language랑 카메라가 들어가면 그대로 action이 나오는 거죠. 똑같이 이렇게 만들어져 있고요.

이거는 NVIDIA GR00T, 아까 거는 N1.6이었고 이건 N1이거든요. 다 비슷하게 생겼어요. 이거 System 1, 2가 분리되기 전에 N1 시절에 VLM에다가 뒤에 Diffusion Transformer 이렇게 바로 들어가도록 되어 있는 거고 이것도 System 1, 2 구조라고 볼 수 있겠네요. 이때는 이런 말을 안 썼었네요.

그리고 SmolVLA도 비슷하게 생겼습니다. VLM이 있고 이건 SmolVLM에다가 action expert 똑같이 Flow Matching 붙여서 action을 output을 continuous한 value로 만든다. 그래서 내용을 쓱 한번 보면 다 똑같이 생겼어요. 그래서 이런 걸 보면 아까 질문 주신 내용이랑 답변이 되는 거죠. 다 크게 다르지 않다. 여기가 막 이렇게 중구난방 다양한 연구들을 하고 있는 것 같지는 않다. 조금씩의 디테일 차이가 있지만

<span class="paragraph-timestamp" data-ts="1:00:07">1:00:07</span> **노정석** 출발들은 조금씩은 다른 게 있었지만 다 이런 방향으로 수렴됐다. vision, System 1, 2 하는 방식으로 이렇게 인지를 담당하는 부분과 action을 담당하는 부분이 갈라졌고.

<span class="paragraph-timestamp" data-ts="1:00:17">1:00:17</span> **최승준** 흥미로운 거는 저는 Diffusion하고 Transformer가 현세대의 작동 원리들이잖아요. 모든 것들에 많은 것들이 그걸 쓰니까 이번 이것도 그거라면 되겠네의 경로, 뭐가 아직은 부족한, 데이터 등이 부족하긴 하지만 되겠네의 경로라는 인상이 들어오네요. 다른 거 예로

<span class="paragraph-timestamp" data-ts="1:00:34">1:00:34</span> **노정석** 되겠네라기보다는 아주 잘 되고 있네.

<span class="paragraph-timestamp" data-ts="1:00:34">1:00:34</span> **최승준** 되고 있네.

<span class="paragraph-timestamp" data-ts="1:00:42">1:00:42</span> **박종현** 저도 비슷하게 생각합니다. 여기도 되네, 이렇게 볼 수도 있을 것 같아요.

<span class="paragraph-timestamp" data-ts="1:00:46">1:00:46</span> **노정석** 그러니까요. 이것 역시 되네.

<span class="paragraph-timestamp" data-ts="1:00:48">1:00:48</span> **박종현** action이 아닌 것도 될 수 있겠다는 생각도 똑같이 더 다른 분야도 다 되겠다는 생각이 똑같이 들기도 하고요.

<span class="paragraph-timestamp" data-ts="1:00:54">1:00:54</span> **최승준** 그 전이 가능한 거죠. 개념들이.

<span class="paragraph-timestamp" data-ts="1:00:56">1:00:56</span> **노정석** 그냥 modality의 확장인 거죠.

<span class="paragraph-timestamp" data-ts="1:01:00">1:01:00</span> **박종현** 그리고 올해 나온 2개, 제가 인트로에서 설명했던 2개 Sharpa랑 이걸 보여드리면 여기도 비슷하거든요. 2개가 촉각을 추가했기 때문에 이거를 Vision-Tactile, VTLA라고 부르는데 여기는 System이 0, 1, 2예요.

<span class="paragraph-timestamp" data-ts="1:01:19">1:01:19</span> **노정석** 0이 들어가 있네요.

<span class="paragraph-timestamp" data-ts="1:01:21">1:01:21</span> **박종현** 0이 하나가 더 생겼어요. 그다음에 촉각은 0에만 들어가는 거죠. 3단계로 분리하고 촉각이라는 거는 정말 빠른 반응에만 필요한 거다, 이렇게 생각을 하고 있는 것 같아요.

<span class="paragraph-timestamp" data-ts="1:01:29">1:01:29</span> **최승준** 그렇구나, 이거는 뭐 더 reflex가 더 강한 그런 것들이네요. 더 근원적으로 올라가는 거네요.

<span class="paragraph-timestamp" data-ts="1:01:36">1:01:36</span> **노정석** 그러나 기본 골격은 비슷하다.

<span class="paragraph-timestamp" data-ts="1:01:39">1:01:39</span> **박종현** 어쨌든 느리게 생각하고 상황 인지하고 사고하고 점점 반응이 빨리 필요한 것들로 내려간다. 그리고 Figure Helix, 이 친구도 들어가 보면 System 0, 1, 2예요.

<span class="paragraph-timestamp" data-ts="1:01:55">1:01:55</span> **노정석** 얘네도 0에 해당하는 어떠한, 그러네요. 0, Human-like Soft Motor Tracking이라고 써 있네요. 그러니까 미묘한 그 뭐라고 그래야 될까요?

<span class="paragraph-timestamp" data-ts="1:02:05">1:02:05</span> **박종현** Stable Motion Tracking인데 이걸 조금만 더 부연 설명을 하자면 예를 들어서 LBM 같은 경우에 System 0 자리가 rule-based였거든요. action 토큰이 나오는데 action 토큰으로 로봇을 컨트롤하게 되면 예를 들면 넘어져요. 아니면 action이 완벽하지가 않으니까 자기 손끝이 부딪치거나 어떤 행동을 하는 데 있어서 잘못된 행동이 나올 수도 있습니다. 보행 같은 경우는 우리가 이미 다 잘 만들어 놨어요. 예전에 RL 기반으로 만들든 아니면 진짜 MPC라고 해서 저도 그런 건 잘 모르는데 로보틱스에서 옛날부터 해오던 어떤 물리적인 방법으로 어디를 디뎌야 중심이 잡힌다, 이런 것들을 계산하는 로직들이 다 있거든요. 그래서 그런 도움을 받아요. 받았어요. 전에는 그래서 action 토큰이 나오고 거기다가 조금 더 제한이나 보정 같은 걸 룰로 둬서 구현하는 방식이 많았는데 이제 그런 걸 없애버리는 거죠. 거기도 그냥 다 모델로 하면 된다, 이런 식으로 접근을 하고 있는 것 같습니다.

그리고 여기도 사이즈를 한번 보시면 System 2가 7B이고요, System 1이 80M이고 이게 Figure 01 때는 이렇게 2개가 있었던 거고 10M짜리 더 작은 친구가 하나가 더 추가가 된 거예요. 그리고 더 작은 친구는 여기에 이 그림에 따르면 real to sim, sim to real 이런 데이터들을 만들어 가지고 시뮬레이션 데이터가 같이 섞여 있다 이렇게 되어 있는데 이게 결국에는 RL이 여기에 들어갔다라는 뜻으로 보이거든요. 제가 느끼기에는 Helix 같은 경우는 오픈소스가 아니라서 저희가 정확하게 어떻게 했는지까지는 모릅니다만 일단 여기까지 해서 VLA가 대충 이런 식으로 수렴 진화를 했고 이런 방향으로 가고 있구나, 대충 다 비슷하게 가고 있구나라는 걸 깨달았고요.

## Physical Intelligence란 무엇인가: Moravec's Paradox    *1:03:48*

<span class="paragraph-timestamp" data-ts="1:03:52">1:03:52</span> **박종현** 그다음에 마지막에 정리하고 싶은 게 Physical Intelligence라는 게 무엇인가, 이거에 대해서 생각을 한번 해보려고 합니다. 결국엔 이 VLA를 만들고 하는 게 Physical Intelligence를 우리가 풀어나가는 과정이다, 이렇게 생각을 해요. LLM으로 우리가 AGI 만드는 게 결국에는 지능을 푸는 과정이다 이렇게 생각을 했는데 여기서 하나 더 분할을 하는 거죠.

저희가 이게 가장 유명한 예시가 있습니다. DARPA Challenge라고 해서 한 10년 전에 있었던 일인데 아무도 문을 못 여는 그런 경우가 있었거든요. 다들 이걸 못하는 거예요. 아니 이게 문 여는 게 뭐가 그렇게 어렵다고 이걸 아무도 이렇게 못하나. 이런 게 밈이 됐었는데 지금도 비슷한 거예요. 이런 걸 Moravec's Paradox, 역설이라고 얘기를 하는데 우리가 체스 같은 거 되게 머리가 필요하고 지능이 필요하고 이런 거라고 생각을 하는데 저희가 주머니에 사탕도 있고 열쇠도 있고 동전도 있고 이런 거 했을 때 열쇠 꺼내는 걸 너무 쉽게 잘하거든요. 아무렇지도 않게. 이런 걸 우리가 지능이라고 보통 안 부르죠. 똑똑함을 얘기할 때 이런 걸 잘 꺼낸다고 똑똑하다고 얘기하진 않잖아요. 그런데 막상 우리가 이런 걸 구현을 하려고 보니까 이게 너무 어렵다는 거예요. 이게 왜 어려울까, 뭐가 다른 걸까.

그래서 제가 어제 찍었거든요. MIT의 김상배 교수님이 로봇을 오래 해오셨던 분인데 제가 이분의 토크를 현장에서 한번 보고 되게 인상이 깊었어서 저도 한번 해봤어요. 제가 눈으로 안 보고 여기서 그냥 핀을 하나 집어요. 그걸 슬로우 모션으로 해 놓은 건데 너무 당연하죠. 사람이라면 그냥 99.9% 성공하는 일입니다. 실패할 리가 없는 일이에요. 그리고 이게 슬로우 모션 돌려서 그렇지 1초도 안 걸리거든요. 찍은 시간이 1초도 안 되는 시간 동안 벌어진 일인데 제가 어떤 말을 딱 집었는데 이 상황에서 딱 봤을 때 제가 무슨 말을 집을지 예상이 되시나요?

<span class="paragraph-timestamp" data-ts="46:21">46:21</span> **노정석** 아니요.

<span class="paragraph-timestamp" data-ts="1:05:51">1:05:51</span> **박종현** 예상이 안 되죠. 우리가 이걸 rule-based로 비전으로 따서 집게로 집는다, 이런 로봇을 만든다고 생각하면 보통 맨 위에 걸 집는 게 최적이라고 생각을 하거든요. 그런데 사람은 그렇게 동작하지 않는다는 거죠. object detection에서 좌표를 따서 그걸로 로봇을 컨트롤해서 무언가 물체를 움직이는 게 이 VLA가 나오기 이전까지 대부분의 로봇이 그렇게 돌아가고 있는데 그 방식이랑 너무 다르다는 거예요. 어쨌든 인간의 Physical Intelligence를 한번 보면 제가 딱 집으려고 할 때 지금 벌써 한 번 실패를 했어요. 아무거나 짚이는 걸 집으려고 했는데 실패를 했고 제 손에서는 촉각이 들어오고 있는데 이 촉각이 엄청나게 많은 큰 차원의 값이 들어옵니다. 이 손에 모든 contact point가 엄청나게 많기 때문에 그다음에 그 촉각을 바탕으로 뭘 집어야겠다 싶은 것들을 무의식적으로 판단을 해서 집어서 하나 짚이는 대로 짚는 거죠.

1초밖에 안 됐지만 엄청나게 많은 데이터 처리와 빠른 의사결정이 그 안에 있었던 거예요. 이런 게 사실 Physical Intelligence라는 거죠.

그래서 제가 슬로우 모션을 보면서 생각을 정리를 해보면 의사결정이 그 안에 사실은 5번이나 있었다, 조금 과장 보태면 5번이나 있었다라는 거고요. 비슷한 예시가 혀예요. 혀라는 게 식사 중에 엄청나게 많은 일을 담당하는데 제가 점심이라고 썼지만 지금 저희는 아침이니까 예를 들어 어제 저녁에 뭘 먹었는가 하면 기억할 수 있지만 그때 혀가 뭘 했는가 하면 사실 하나도 기억이 안 난다는 거죠. 그냥 알아서 한다는 거죠.

그래서 이거는 Cognitive Intelligence랑은 다른 거다. 어떤 reasoning token 같은 게 나오면서 생각을 하고 이런 거랑은 완전히 다른 지능이다, 이런 생각을 하게 되고요. 이거는 다 교수님께서 하시는 토크를 제가 듣고 설득이 된 내용들입니다. 내용은 한번 보시는 걸 추천드립니다. TED에도 있고 엄청나게 많은 곳에서 많은 강연들을 하셔서 지능에 대해서 저희가 생각해 볼 수 있는 좋은 계기를 잘 설명을 해 주시고요. 모든 걸 동의하지는 않아서 제가 동의하는 부분만 뽑아 왔습니다. 그럼 이런 게 왜 발생하나라고 생각을 해보면 진화적인 관점에서 설명을 하는 사람들이 있더라고요. 저희가 이런 운동, Physical Intelligence는 10억 년 동안 진화를 해서 만들어진 인간 말고도 많은 동물들이 다 가지고 있는 그런 능력인데 체스나 바둑 혹은 추상적인 수학 같은 거는 진화적인 관점에서 보면 그렇게 오랫동안 걸려서 만들어진 능력은 아니라는 거예요. 그래서 어쩌면 이게 더 당연하다고 생각하지만 어려운 거일 수도 있지 않을까라는 생각을 하게 되고요.

그래서 다람쥐가 날아다닐 때 뉴턴 역학을 계산해서 날아다니지 않는다는 거죠. 그런데 VLA 이전에 있었던 그런 MPC라는 알고리즘들 같은 경우에는 이런 역학들을 계산해서 어디에다가 힘을 어떻게 주면 어떻게 딱 나가겠다, 정말 완벽하게 행동들을 할 수 있는데 우리의 뇌에서 동작하는 Physical Intelligence랑 다르다는 거죠. 그다음에 차원과 속도의 관점에서 생각을 한번 해보면 사람 같은 경우에는 촉각이라는 게 있는데 이 비전이나 촉각 같은 물리적인 정보는 차원이 엄청 커요. 그런데 우리가 텍스트, language의 세상은 토큰들이기 때문에 토큰이 저희가 tokenizer 지금 쓰는 게 한 200K 되는 그런 토큰들 쓰잖아요. 20만 가지 중에 한 가지를 선택하는 것들의 연속으로 language를 우리가 취급을 하고 있는데 상당히 abstract 되어 있다. 차원이 엄청 작다는 거죠. 비전이나 촉각에 비하면. 그래서 애초에 처리해야 될 데이터가 너무나 크다라는 거고 텍스트의 세상은 이미 너무 효율적인 세상이에요. 어려운 사고, 학습 이런 데 오히려 상당히 효율적이에요.

쓸데없는 정보가 하나도 없어요. 그래서 우리가 RLVR 같은 걸 해가면서 이런 게 지능, 똑똑함을 만드는 데 연구를 하는 지능을 만드는 데 필요하겠다, 효율적인 방법이다. 언어가 상당히 중요하다 이렇게 생각을 하지만 사실 언어라는 거는 거의 인간만 완벽하게 쓰고 있기 때문에 저런 물리적인 것들은 좀 다르다는 거죠.

게다가 이건 나노 바나나를 만들었던 Google Developers 채널에 나왔던 나노바나 팀의 얘기에서 제가 봤던 건데 이 언어에는 Reporting Bias라는 게 있다고 하거든요. 예를 들어서 제가 어제 다른 집에 놀러 갔다 왔어요. 그러고 와서 집에서 그걸 글로 쓰는 거예요. 어땠나. 제가 어제 다른 회사에 미팅을 갔다 왔는데 건물이 되게 으리으리하고 바다가 보였고 이런 이야기를 해요. 제가 인상 깊었던 것들을 이야기를 하죠. 그런 것들은 데이터가 남아 있어요. 그런데 제가 그 건물의 벽이 흰색이었는지 혹은 상아색이었는지 혹은 의자가 어떻게 생겼는지 이런 얘기를 안 하거든요. reporting을 안 합니다. 왜냐하면 그건 당연하니까. 만약에 의자가 엄청 특이한 모양이었다, 벽 색깔이 되게 특이한 장식이 있었다, 그럼 하겠죠. 되게 특별한 것들, 유의미한 정보들만 텍스트 세상에서는 남아 있어요. 저희가 LLM에서도 비슷한 문제가 있거든요.

텍스트를 다 배웠는데 분명히 인터넷 스케일로 데이터를 다 배웠는데 지식을 다 배웠는데 코끼리 냉장고에 어떻게 넣어, 그러면 안 된다 해야 되는데 얘가 넣으라 그래요. 문 열고 넣으면 됩니다. 왜냐하면 인간한테 너무나 당연했던 코끼리는 엄청 크니까 냉장고에 절대 안 들어가지, 이런 게 텍스트로 써져 있지 않거든요. 텍스트 세상이라는 게 사실은 상당히 누락된 정보가 많다는 거예요. 기본적인 누락된 정보가. Physical Intelligence라는 거는 그런 정보들을 다루는 문제다. 그래서 어쩌면 조금 다른 차원의 문제다, 이런 생각을 하게 됩니다.

그래서 결론을 하자면 Physical Intelligence라는 거는 좀 다르다는 거고요. 그다음에 실제로 AI가 LLM도 마찬가지지만 인간과 학습 방법도 조금 더 다르다는 거고 사람은 어떤 경험을 통해서 이런 것들 특히 물리적인 거는 글로 배운다기보다는 대부분 다 경험을 통해서 많은 것들을 배우게 되는데 그런 것들이 구현이 지금 잘 안 되기 때문에 좀 어려울 수도 있다라는 얘기고요. 그래서 그럼 잘 안 된다는 건가라는 마지막 질문을 던져보면 저는 될 것 같다고 생각해요. 왜냐하면 지금 이런 어려움들이 있는데 저희가 해결할 수 있는 방법이 너무나 많고요. LLM도 마찬가지고 여기도 마찬가지로 꼭 사람이 배우는 방식과 똑같이 배울 필요가 없다고 생각합니다. 사람이 잘하는데 얘가 못하는 게 있겠죠. 그래도 우리 세상을 바꿀 만큼 Physical Intelligence를 충분히 달성이 가능하다. 이렇게 생각하고 그 대표적인 예시가 촉각이라는 게 구현이 어려워서 대부분 손바닥이나 손등에 요즘 휴머노이드를 보면 카메라가 달려 있거든요. 그런 비전 정보로도 무언가 행동을 해낼 수 있는 거죠.

사람은 눈이 얼굴에밖에 없지만 손바닥에다 눈을 달 수 없지만 로봇은 달 수 있으니까. 자동차 자율주행도 마찬가지로 LiDAR 같은 거 달아서 사람은 잘 못하는 그런 거리감 같은 데이터를 받아서 해결을 할 수가 있으니까 어떤 방식으로든 충분히 달성 가능할 것 같다. 스케일링만 할 수 있다면 저는 이렇게 낙관적으로 생각을 하고 있습니다. 여기까지 마무리하겠습니다.

<span class="paragraph-timestamp" data-ts="1:13:23">1:13:23</span> **노정석** 오늘 전체적인 개괄에 대해서 말씀 주시고 그다음에 어떤 변화들이 있었는지, 그리고 그 각각의 개괄 영역들에서 잘하고 있는 회사들, 연구 기관들, 그리고 모델의 구조들, 그리고 그 주변을 둘러싼 여러 가지 문제점과 철학적인 질문들 이런 거 다 커버를 해 주셨고요. 그리고 맨 마지막에는 짧게 말씀하셨습니다만 이것도 결국 금방 되는 게임이다. 여기서 플레이하고 있는 스타트업들 화이팅이고 이제 진입하시고자 하는 수많은 분들, 이 부분은 LLM과는 달리 이 각각의 도메인이라고 불러야 될지 아니면 그냥 last mile problem이라고 불러야 될지에 대해서 제가 요새 그 개념이 조금 헷갈리긴 합니다만 왜냐하면 도메인이라고 하더라도 남은 건 다 last mile만 남아 있거든요.

앞에 있는 것들은 general intelligence들이 다 끝내고 있어서 그 수많은 last mile들이 여전히 기회로 남아 있기 때문에 이거 꽤 올해 내년 핫하다, LLM 열차에 탑승하지 못한 인재들은 가볼 만하다, 이런 정도 생각이 들고 있습니다.

## 사업 방향: 커뮤니티 전략과 게임 시뮬레이션    *1:14:27*

<span class="paragraph-timestamp" data-ts="1:14:30">1:14:30</span> **박종현** 저도 정확하게 그 생각으로 이걸 해야겠다라고 생각하고 있습니다.

<span class="paragraph-timestamp" data-ts="1:14:35">1:14:35</span> **노정석** 종현 님은 어떤 아이디어를 가지고 계신가요? 종현 님이 사실 이 VLA 쪽을 다 트래킹을 하시고 이런 쪽에 무언가 기회가 있을 것 같다라는 그 느낌들을 계속 갈고닦고 계시잖아요. 그러면서 지금 회사들은 어디로 가고 있는지, 저희 표현에 따르면 다들 어디로 도망들 갔잖아요. 가서 하나씩 하고 있는데 종현 님은 어떤 포지션에서 하실 건지 그런 거 궁금합니다. 한번 개인적인 의견들, 개인적인 방향성, 일종의 사업 아이디어 이런 거 있으시다면 한번 말씀해 주실 수 있을까요?

<span class="paragraph-timestamp" data-ts="1:15:10">1:15:10</span> **박종현** 제가 하고 싶은 거는 지금으로서는 크게 한 두 가지 정도 있는데 일단 이길 수 없으면 합류하라라는 말이 있지만 LLM도 저희가 큰 플레이어들이 스케일링 해 가는 걸 따라가기가 사실 쉽지 않잖아요. 여기도 마찬가지일 거라고 생각해요. 큰 플레이어들의 스케일링을 따라가기는 거기에 합류하지 않으면 안 될 것 같고요.

그러면 걔네들이 하지 않는 무언가 파트들이 있을 텐데, 첫 번째는 저는 HuggingFace의 전략을 개인적으로 동의하기도 하고 좋아하는 전략이에요. 커뮤니티가 이길 수도 있다, 이런 생각을 하고요. 그다음에 결국 로봇이라는 게 LLM과 조금 다른 점은 몸체가 비싸거든요. 이 몸체를 싸게 만들어서 배포를 하고 대중화를 하는 일을 해볼 수도 있겠다, 이런 생각을 첫 번째로 하고 있고요. 거기에는 몸체만 싸게 만드는 것뿐만 아니라 사람들이 일상적으로 쓸 만한 task를 VLA로 만들어서 그런 걸 제공하는 걸 같이 해야겠죠. 그리고 그 데이터를 다 같이 커뮤니티가 모아서 모든 사람들이 다 같이 지능을 올리는. 모든 사람이 다 기여하는 그런 방식으로 갈 수도 있겠다는 생각을 하고요.

또 하나 다른 방향은 저는 어렸을 때부터 게임을 좋아했기 때문에, 이 게임이라는 게 사실 물리 시뮬레이션과 비슷한 부분이 상당히 많아서 게임 세상 속에서의 어떤 action, 그게 요즘에는 world model로 많이 표현이 되고 있는 것 같아요. World model일 수도 있고 물리 시뮬레이션일 수도 있고, 이 게임이 돌파구가 될 수도 있겠다. 그런 가상 세상과 현실 세상을 잇는 그런 작업들, 데이터 문제도 해결하고 무엇보다 evaluation 문제가 해결이 되거든요. 실생활에서 로봇을 evaluation 한다는 게 너무나 비싼 일이어서, 그런 방향 속에서도 기회가 많지 않을까 이렇게 생각을 하고 있습니다.

<span class="paragraph-timestamp" data-ts="1:17:06">1:17:06</span> **노정석** 연결되는 지점들이 있는 것 같은데, 둘 다 그 돈으로 해결할 수 없는 스케일을 해결하는 방향성들에 대해서 말씀해 주신 걸로 저는 좀 받아들여지고요. 그럼 종현님, 그 첫 번째 어떤 커뮤니티와 함께 가는 방법 이런 걸 했을 때, 아까 말씀하셨던 것처럼 SmolVLA 뭐 그다음 이런 모델들이 오픈 모델이잖아요. 그 저희가 이쪽에 한번 뛰어들고 싶은 사람들 입장에서 지금 시작하기에 어떤 공부의 경로를 좀 알려주시면, 가장 처음으로 어떤 paper를 한번 읽어보는 게 좋겠다, 그다음에 두 번째로는 어떤 하드웨어 폼팩터를 가지고 어떤 커뮤니티에서 시작해 보는 게 좋겠다, 관심 있는 사람 있으면 나한테 와라 뭐 이런 등등 여러 가지 가이드가 있으실 것 같은데, 가이드 좀 주세요. 공부를 어디서부터 시작해야 됩니까?

## 입문 가이드: LeRobot, Physical Intelligence 논문    *1:17:10*

<span class="paragraph-timestamp" data-ts="1:17:53">1:17:53</span> **박종현** 리서처가 되고 싶으면 좋은 하드웨어가 필요하니까 사실은 합류를 해야 되는 게 맞는 것 같아요. 큰 회사에 혹은 연구실, 연구 조직. 그런데 그런 게 없어도 충분히 다 이런 걸 follow-up 할 수 있거든요. 왜냐하면 오픈소스 하드웨어들이 있기 때문에.

저는 HuggingFace의 LeRobot이 저도 그렇게 처음에 딱 입문을 했고 제일 좋은 시작 지점이라고 생각을 하고요. 로봇 같은 경우에는 우리나라에도 로보티즈라는 회사에 오픈소스 로봇이 있습니다. 그냥 3D 프린터로 뽑아서 조립할 수 있어요. 한 50만 원 정도면 구매가 가능해서, 이 텔레오퍼레이션 시스템까지 다 되어 있거든요. VLA를 직접 fine-tuning 해볼 수 있는 거를, 제가 처음에 했을 때는 우리나라 건 없었고 HuggingFace 것만 있었는데 한 이틀 걸렸어요. 저희가 하는 데, 로봇 사서 조립해서 텔레오퍼레이션에서 데이터 따고 실제로 VLA 학습, 그땐 VLA는 아니고 그냥 language는 없는 vision-action 모델이었는데 fine-tuning 해서 실제 task 시키는 것까지 한 이틀 정도 걸렸거든요. 누구나 사실 다 해보실 수 있습니다.

그래서 직접 해보시는 거 좋아하시는 분들은 한 50만 원 정도 가지고 시작하실 수 있다. 그다음에 저희가 이 페이지, 제가 공유를 드릴 건데 이 페이지에 제가 그런 안내를 추가하려고 합니다. 저희가 그런 걸 다 했었고 그래서 튜토리얼 같은 자료들을 넣으려고 하고요.

그리고 연구를 해보고 싶으시면 시뮬레이터에서도 충분히 할 수 있어서, NVIDIA의 Isaac Sim 문서 참고하시면 되고 그다음에 paper는 Physical Intelligence 회사 가서 paper 쭉 한번 보시면 대충 흐름이 딱 보입니다. 어쨌든 선두 주자이고 공개도 꽤 많이 해주기 때문에 Physical Intelligence 그냥 페이지를 추천드립니다. Paper들 다 거기 있습니다.

<span class="paragraph-timestamp" data-ts="1:19:44">1:19:44</span> **노정석** 알겠습니다.

## 로봇이 우리 삶에 들어오는 미래와 클로징    *1:19:45*

<span class="paragraph-timestamp" data-ts="1:19:51">1:19:51</span> **최승준** 저는 오늘 얘기 쭉 듣고 나서 생각이 또 복잡해지는데, 그러면 결국에 이러한 기술의 발전이나 실제로 구현이 일어나면 몇 년 뒤에 우리의 삶 속에 로봇이 들어오게 되는 건가요? 어떤 폼팩터에 어떤 제품이나, 아니면 걔네들이 해야 될 일이 뭔가 이런 것들을 좀 상상해 보게 되는 지점인 것 같아요.

근데 맥락을 좀 더 하자면, 지금 당연히 너무 생각하는 거는 일종의 노동을 맡기는 그런 거잖아요. 근데 그것만일까, 그런 거를 좀 생각해 보게 되더라고요.

근데 종현님은 어떻게 생각하세요?

<span class="paragraph-timestamp" data-ts="1:20:19">1:20:19</span> **박종현** 일단 로봇이 어떤 형태로든지 간에 우리 삶에 들어올 거는 저는 확실하다고 생각을 하고요. 몇 년 안에. 다만 문제는 가격과 양산의 문제인 것 같아요. 그건 저도 잘 모르기 때문에, 어떤 어려움이 있는지 어떤 해결이 쉽게 되는 건지 이런 걸 잘 모르지만 무조건 누가 봐도 사실 시장 가치가 너무 커서 아마도 노동 시장부터 시작을 할 것 같고요. 그다음에 가정용도 충분히 들어올 것 같고, 그 로봇의 형태가 꼭 휴머노이드일 것이냐에 대해서는 저는 꼭 그렇지는 않을 수도 있다고 생각을 합니다. 예를 들어서 책상에 로봇이 하나씩 있을 수도 있는 거고요. 아니면 로봇 인형 같은 게 있을 수도 있는 거고요. 여러 가지 형태로 그런 것들이 나올 수 있을 것 같고, 싱크대에 하나씩 달려 있을 수도 있겠죠. 그냥 팔 같은 게 하나씩 달려서. 어쨌든 여러 가지 형태로 어떤 형태로든지 생길 거라고 보고요.

그다음에 제가 상상하기로는 지금으로서는 생각하지 못하는 완전히 새로운 형태의 환경이 바뀔 거라고 생각해요. 그러니까 지금 모든 우리의 어떤 가구, 집의 모양, 사무실의 형태 다 인간 폼팩터에 맞춰져 있거든요. 문의 폭 이런 것들이. 근데 사실 인간만 할 수 있는 일을 많은 부분을 로봇이 하게 된다면, 지금의 엄청 massive한 공장들처럼 앞으로 생기게 될 로봇의 폼팩터에 맞는 물건과 도구와 어떤 공간들이 만들어질 거라고 생각을 합니다. 예를 들어서 카페 같은 데에 지금은 인간 통로의 음식점에 로봇이 돌아다닌다면, 그게 아니라 로봇을 위한 다른 작은 레일 같은 게 깔릴 수도 있는 거고.

<span class="paragraph-timestamp" data-ts="1:21:55">1:21:55</span> **최승준** 병원 같이 그런.

<span class="paragraph-timestamp" data-ts="1:22:02">1:22:02</span> **박종현** 예를 들면 병원 같이. 그럼 걔네가 다 서빙도 하고 가져가고 설거지하고 레일 타고 로봇이 돌아다닐 수도 있는 거.

어쨌든 지금은 좀 상상하기 어려운 많은 형태의, 특히나 환경 변화가 저는 생길 거라고 추측을 하고요. 어떤 인간의 많은 원초적인 욕구들을 해결해 주는 방식으로 들어오겠죠. 그거는 노동 해방일 수도 있고 어떤 요리를 잘 해 주는 그런 거일 수도 있고요. 아니면 성적인 거일 수도 있고, 많은 형태로 점점 더 아래 레벨로 내려갈 거라고 생각을 합니다.

<span class="paragraph-timestamp" data-ts="1:22:31">1:22:31</span> **노정석** LLM에서 일어났던 변화들과 비슷한 형태의 어떤 분화, 무한 발전이 이쪽도 그냥 당연히 생긴다라는 말씀을 계속 주시고 있는 것 같아요.

<span class="paragraph-timestamp" data-ts="1:22:41">1:22:41</span> **최승준** 여기 sudoremove RF가 자꾸만 눈에 들어와요.

<span class="paragraph-timestamp" data-ts="1:22:49">1:22:49</span> **박종현** 비슷한, 지금 와서 보면 비슷한 의미인 것 같아요. 저희 채널의 sudoremove라는 게 개발자분들은 다 아시겠지만 다 지운다, 이런 뜻이잖아요. 새로운 게 나왔으니 환경도 예를 들면 집도 가구도 다 지우고 새로운 세상에 맞는 걸 만들어야 된다. 우리의 뇌 속에 있는 지식도 사고 방식도 뭐 이런 뜻입니다.

<span class="paragraph-timestamp" data-ts="1:23:08">1:23:08</span> **최승준** 하여튼 뭔가 개괄을 쭉 한번 풀어내 주셔서 저희도 컨텍스트에 이게 좀 들어오고, 다음에 만나면 조금 더 깊이 들어가서 얘기를 한번 해볼 수 있을 것 같다는 생각이 듭니다.

<span class="paragraph-timestamp" data-ts="1:23:17">1:23:17</span> **노정석** 저희도 오늘 소개해 주신 SmolVLA죠. SmolVLA랑 Physical Intelligence 그런 쪽 paper와 이런 쪽을 좀 더 트래킹하고 또 한 번 가르침을 좀 청하도록 하겠습니다.

<span class="paragraph-timestamp" data-ts="1:23:33">1:23:33</span> **최승준** 다양한 관점들을 배울 수 있었어요. 특히나 아까 System 2에서 올라가는 건 생각도 못했었거든요, 저는. 감사합니다.

<span class="paragraph-timestamp" data-ts="1:23:36">1:23:36</span> **박종현** 큰 형님들끼리 다 연구해 주신 거 그냥 대신 읊어드릴 뿐입니다.

<span class="paragraph-timestamp" data-ts="1:23:43">1:23:43</span> **노정석** 참 재미있는 세상에 살고 있는 것 같습니다. 오늘은 저희 그 sudoremove의 박종현님과 함께 AI 프론티어, sudoremove 이 합방이 되겠습니다. 오늘 가르쳐 주셔서 너무 감사드리고요. 너무 재밌게 잘 배웠습니다. 감사합니다.

<span class="paragraph-timestamp" data-ts="1:23:59">1:23:59</span> **박종현** 수고하셨습니다. 감사합니다.
